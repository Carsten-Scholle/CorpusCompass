{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tool is meant for researchers in the field of Corpus Linguistics and work on e.g. language variation.\n",
    "\n",
    "Given your corpus and a list of variables, this code will build a structured dataset (in `csv`) for your future analysis. Moreover, it will help you to spot inconsistencies into your annotation and provide a clear overview of how your annotated features correlate with speakers. \n",
    "\n",
    "While we do have a standard for annotations (ELAN and Praat), instructions to adapt the code to your own syntax are available below.\n",
    "\n",
    "If you find any problems or request some features update, please do so with [this form](https://github.com/nicofirst1/CorpusCompass/issues/new/choose).\n",
    "\n",
    "## How does it work?\n",
    "\n",
    "This file you are viewing is called a google Colab. It allows you to run the code (python in this case) in an interactive manner by clicking on each `code cell`. If you are not familiar with google Colab, we advise you to check the [tutorial](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "\n",
    "When you run each cell, you will see (most of the time) some additional information appearing regarding the status of the program, e.g. number of annotated words, varaibles, etc ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Run this to copy the necessary files in your colab.\n",
    "If it says something like:\n",
    "```fatal: destination path 'CorpusCompass' already exists and is not an empty directory.```\n",
    "You can ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/nicofirst1/CorpusCompass\n",
    "!pip install nltk\n",
    "!pip install pandas\n",
    "!pip install -e CorpusCompass/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import itertools\n",
    "from copy import copy\n",
    "\n",
    "from CorpusCompass.src.dataset_creator.utils import *\n",
    "\n",
    "import pandas as pd\n",
    "from google.colab import files\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom files and settings\n",
    "\n",
    "In this section, we will define some settings for your transcription!\n",
    "\n",
    "We will start with the variables you defined and move on to how to find them in your annotated corpus (REGEX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Investigating language variation means that there is more than one way of saying the same thing. Speakers may vary pronunciation, morphology, word choice, etc.\n",
    "\n",
    "In linguistic research, we usually work with a number of variables (dependent and independent). \n",
    "In simple terms, an independent variable is the “input” (controlled factor) and a dependent variable is what results from the set of independent variables as an \"output\" (outcomes being measured). On the one hand, an independent variable is what is given (e.g. age, sex, education). On the other hand, a dependent variable is what results from the set of independent variables (e.g. pronunciation of a phoneme, morpheme or words).\n",
    "\n",
    "### Defining variables with JSON\n",
    "When annotating your data, you will for sure use some variables. These variables may have a hierarchical structure, where one category includes many variations. To allow the program to find your variables you have to build a \"dictionary\" where you specify them.\n",
    "Here, we use JSON files that allow you to come up with how many categories and variables you want in a clear and defined manner. Check out the [introduction to JSON tutorial](https://www.w3schools.com/js/js_json_intro.asp) if you are not familiar with it.\n",
    "You can also look at the  [dependent variables](./dependent_variables.json) we are using in this project for an example.\n",
    "\n",
    "### Your files\n",
    "Now that you are familiar with how the JSON syntax work, we need to define what your variables are. Here you have two options, depending on the option you choose you will need to run different cells.\n",
    "\n",
    "##### 1. Use the default variables file\n",
    "At the moment there are two files, one for [dependent variables](./dependent_variables.json) and one for [independent variables](./independent_variables.json) in this code. If you open these link you will see the original files (not modificable). If you want to modify them you need to open them from this page. On the folder symbol on the left of this code (check [this video](./includes/changing_variables.gif) for a how-to, and  [this tutorial](https://neptune.ai/blog/google-colab-dealing-with-files) on how to access local files system to google colab, point 4), click on CorpusCompass (this will open the direcotry) and then you will see the two files. Click on them to modify their content.\n",
    "\n",
    "##### 2. Upload your own files\n",
    "Here you can upload your own variable files as long as they are still in JSON format. You can upload as many files as you want. Be sure to upload the dependent variable first!\n",
    "\n",
    "##### Choose your method\n",
    "You can choose either method 1 or 2, by changing the value of `variable_method` to be 1 or 2. By defualt the method is set to 1 (using the variables defined here).\n",
    "\n",
    "It should look like this `variable_method=1` or this `variable_method=2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment for different methods\n",
    "\n",
    "variable_method = 1\n",
    "\n",
    "assert variable_method in [1, 2], f\"Invalid method number {variable_method}! Choose between 1 and 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if variable_method == 1:\n",
    "    # method 1, load from files\n",
    "    dependent_variable_path = 'CorpusCompass/dependent_variables.json'\n",
    "    independent_variable_path = 'CorpusCompass/independent_variables.json'\n",
    "\n",
    "    variable_files = [dependent_variable_path, independent_variable_path]\n",
    "    variable_files = [open(path, \"r+\") for path in variable_files]\n",
    "    variable_files = [x.read() for x in variable_files]\n",
    "else:\n",
    "    # method 2, upload files\n",
    "    variable_files = files.upload()\n",
    "\n",
    "    for fn in variable_files.keys():\n",
    "        if \".json\" not in fn:\n",
    "            raise FileNotFoundError(f\"File {fn} is not a JSON file!\")\n",
    "        print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "            name=fn, length=len(variable_files[fn])))\n",
    "\n",
    "    variable_files = list(variable_files.values())\n",
    "\n",
    "    # decode\n",
    "    variable_files = [x.decode(\"utf8\") for x in variable_files]\n",
    "    if len(variable_files) != 2:\n",
    "        raise ValueError(\"You need to upload two files, one for dependent and one for independent variables!\")\n",
    "\n",
    "# show variables\n",
    "variable_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions (REGEX)\n",
    "\n",
    "Now it's time to define the regular expressions (REGEX) to find your annotation in the corpus.\n",
    "\n",
    "In this example, each annotation is contained in square brackets and starts with a dollar sign. The variables are divided by a dot (without spaces) and the last element after the dot is the annotated word (with spaces).\n",
    "Formally : \n",
    "\n",
    "```[$variable1.variable2.annotated word]```\n",
    "\n",
    "An example: \n",
    "\n",
    "`[$verb.ing.playing]`\n",
    "\n",
    "\n",
    "If your annotations follow a different rule, you need to come up with a new REGEX.\n",
    "\n",
    "You can use [regex101](https://regex101.com/) for this. Follow these instructions to test it:\n",
    "- Open  [regex101](https://regex101.com/)\n",
    "- Copy-paste the content of the `square_regex` (defined below) (`(\\[\\$[\\S ]*?\\])`) into the regular expression bar (on the top)\n",
    "- Copy paste a sample paragraph in the text string (on the bottom), e.g.:\n",
    "```\n",
    "A: akiid aani asawwi  [$G-JOB.awsbildung]  bass igulluuli innu l ʕaluum il baaylooji yaʕni li huwwa taħlilaat ṣaʕub bass iða ṣaʕub asawwi ɣeera akiid yaʕni, innu musaaʕadat doktoor asnaan ħaaba  [$TYP-IA.hammeen], haaða ʃ ʃii [$TYP-IA.hamma] aah, ey waḷḷa, aḷḷa ysallmič  ʕindi tlaθ aṭfaal, aah, il ʕindi ṭifla čibiira sitt isniin bi l madrasa uw ʕali ʕumra tlaθ isniin, arbaʕ isniin [$G-EDU.kindargaartin] uw samiir santeen uw nuṣṣ, santeen uw θmann iʃhuur [$G-EDU.kindarkriipa]\n",
    "\n",
    "```\n",
    "\n",
    "Define our REGEXs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex to find the complete annotation rule\n",
    "square_regex = re.compile(r\"(\\[\\$[\\S ]*?\\])\")\n",
    "# regex to find the content of an annotation\n",
    "feat_regex = re.compile(r'\\[\\$([\\S ]*?)\\]')\n",
    "# regex to univocally finding the speaker name in the paragraph\n",
    "# uncomment if you don't have speakers at the start of each paragraph\n",
    "# name_regex= re.compile(r\"^\")\n",
    "name_regex = re.compile(r\"(^[A-Z]): \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus file\n",
    "Finally, you need to upload your corpus file. This is the file containing your annotated tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change use_example_corpus to False, in order to upload your own corpus\n",
    "# use_example_corpus=False\n",
    "use_example_corpus = False\n",
    "corpus_dict={}\n",
    "\n",
    "if use_example_corpus:\n",
    "    import pathlib\n",
    "\n",
    "    cur_path = pathlib.Path().resolve()\n",
    "    corpus_path = cur_path.joinpath(\"CorpusCompass/includes/corpus_example.txt\")\n",
    "    with open(corpus_path, \"r+\", encoding=\"utf8\") as f:\n",
    "        corpus_dict[corpus_path] = f.read()\n",
    "else:\n",
    "    corpus_path = files.upload()\n",
    "    corpus_dict = multi_corpus_upload(corpus_path, encoding=\"utf-16-le\")\n",
    "corpus_text = \"\\n\".join(corpus_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"The corpus has {} characters\".format(len(corpus_text)))\n",
    "print(\"The corpus has {} paragraphs\".format(len(corpus_text.split(\"\\n\"))))\n",
    "print(\"The corpus has {} tokens\".format(len(corpus_text.split(\" \"))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output file\n",
    "\n",
    "In order to obtain the comma separated values (`csv`) file, you can specify the separator. By default the separator value is a comma (as the name implies), but you can also use:\n",
    "- semicolon: `;`\n",
    "- comma: `,`\n",
    "- tab : `\\t`\n",
    "\n",
    "Be aware that having the same character in your corpus may break the `csv` visualization. We suggest using a symbol that does not appear in your corpus, and then manually set the separator in the program you are using to visualize (e.g. excel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = ';'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program\n",
    "Time to start the main program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing transcriptions\n",
    "Based on your type of annotated corpus you will need to preprocess the file.\n",
    "Feel free to comment out (add `#` at the start of the line) any preprocessing that does not fit your criteria.\n",
    "To give you a sense of our transcriptions, here is how the first paragraphs look like:\n",
    "\n",
    "```\n",
    "A: akiid aani asawwi  [$G-JOB.awsbildung]  bass igulluuli innu l ʕaluum il baaylooji yaʕni li huwwa taħlilaat ṣaʕub bass iða ṣaʕub asawwi ɣeera akiid yaʕni, innu musaaʕadat doktoor asnaan ħaaba  [$TYP-IA.hammeen], haaða ʃ ʃii [$TYP-IA.hamma] aah, ey waḷḷa, aḷḷa ysallmič  ʕindi tlaθ aṭfaal, aah, il ʕindi ṭifla čibiira sitt isniin bi l madrasa uw ʕali ʕumra tlaθ isniin, arbaʕ isniin [$G-EDU.kindargaartin] uw samiir santeen uw nuṣṣ, santeen uw θmann iʃhuur [$G-EDU.kindarkriipa]\n",
    "\n",
    "B: sawweet yaʕni baarħa innu gidarit adrus bi l leel ʕala muud awaffir waqti l yoom ilkum  ayy, ʃukran ilič tislamiin  la, [$G-DL.rootiin] il yoomi ʕindi dawaam aani daaʔiman min iθ θmaaniya w nuṣṣ li s saaʕa arbaʕa asawwi [$G-JOB.awsbildung] tamriiḍ li huwwa [$G-JOB.kraankenbifleegaaa] baʕd id dawaam arjaʕ mumkin asawwi [$G-DL.aaynkawfin] loo maθalan asawwi [$G-DL.teermiin] aw ʃii baʕadeen arjaʕ li l beet, mumkin aṭbux, aakul, leen aani saakna waħdi fa ʃwayya tkuun ṣaʕba bi n nisba ili\n",
    "\n",
    "C: ħiluw aʃyaaʔ [$TYP-IA.aku] ħilwa aa bass atmanna n naas yaʕni, aa [$TYP-IA.Q-WORD.ʃoon]  yistaxdimuuha b ṣuura ṣaħiiħa ey haaða yaʕni il mafruuḍ il kull tfakkir bii ey hm, naʕam, naʕam, naʕam, hm bali, ṣaħħ naʕam, ey ey ey, ṣaħiiħ, ey ṣaħħ, ṣaħiiħ ey ħamd-il-laa ey, naʕam, hmm la, la ṭabʕan akiid, ma, yaʕni ṣaar, aa,  [$TYP-IA.fadd]  ʃii wiyya l ħayaat yaʕni [$TYP-IA.Q-WORDʃoon] ka anna ma ʃurbat il ṃayy ma tgidriin, yaʕni aani ḍiʕit\n",
    "\n",
    "D: yaʕni n naas aa, mitfahhimiin iʃ ʃii uw yaʕni ma [$TYP-IA.da] ydaxxiluun iʃ ʃaɣḷaat yaʕni ḍiddhum haaði fa huwwa l ʕeeb, il ʕeeb bi l baʃar illi il ʕarabi il [$TYP-IA.da] yiji ey, aha, bi ḍ ḍaBiṭ, naʕam bi ḍ ḍaBiṭ la waḷḷa [$TYP-IA.da] ruuħ aa, bass aa l [$G-EDU.koors] xiḷas, ey ma ṭaḷḷaʕit natiija yaʕni, ey ey, ey waḷḷa, waḷḷa yaʕni aa, min, aa xijalit min nafsi yaʕni, gumit aa, ħatta ħatta nafsiiti tiʕbat yaʕni aa, yaʕni ħatta l ħijiyya uw aṣdiqaaʔi yaʕni, igulli ʕammu [$TYP-IA.Q-WORD.ʃinu] yaʕni\n",
    "\n",
    "```\n",
    "As you can see our file has a repeating structure of the kind:\n",
    "- speaker name (`A`,`B`,`C`,`D`), space, paragraph, newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def preprocess_corpus(ct, path):\n",
    "    # step 1: split the whole corpus in different elements every new line, creating a list of paragraphs. For us this means splitting interviewer and interviewee in different paragraphs\n",
    "    c = split_paragraphs(ct)\n",
    "    # step 2: remove spaces at the start and end of each paragraph\n",
    "    c = [x.strip() for x in c]\n",
    "    # remove extra spaces in the middle of the paragraph\n",
    "    c = [re.sub(r'\\s+', ' ', x) for x in c]\n",
    "    # step 3 : remove empty paragraphs from the list\n",
    "    c = [x for x in c if x != '']\n",
    "    # step 4 : filter out all the paragraphs that do not have any DETECTED speaker\n",
    "    prev_c = copy(c)\n",
    "    c = [x for x in c if get_name(x, name_regex)]\n",
    "\n",
    "    removed_sentences = len(prev_c) - len(c)\n",
    "    if removed_sentences > 0:\n",
    "        print(f\"\\n\\n- I removed {removed_sentences} (out of {len(c)}) paragraphs from the '{path}' file, since I could not detect a speaker\\n\"\n",
    "              f\"I will show it over here, sorted by the their line:\")\n",
    "        diff = set(prev_c) - set(c)\n",
    "        diff = sorted(diff, key=lambda x: prev_c.index(x))\n",
    "        for i in diff:\n",
    "            print(f\"{prev_c.index(i)}: {i}\")\n",
    "        print(\n",
    "            \"\\nIf you see paragraphs you are interested in, consider manually changing them in the ccorpus, or expanding the 'name_regex' rule\")\n",
    "\n",
    "    if len(c) == 0:\n",
    "        print(\"All the paragraphs in the corpus have been deleted! You should review your regex rules\")\n",
    "\n",
    "    return c\n",
    "\n",
    "corpus_dict = {k: preprocess_corpus(v, k) for k, v in corpus_dict.items()}\n",
    "corpus = list(corpus_dict.values())\n",
    "corpus = list(itertools.chain(*corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speaker of interest\n",
    "As you can see form the previous example, we consider a file where we have two speakers alternating each other. But it can be the case where you have multiple in random order. In this case you can specify the names of the speakers you are interested in, notice that the others will be skipped from the final output.\n",
    "\n",
    "If you need to add multiple speakers run the following cell after adding the names. It should look something like:\n",
    "\n",
    "\n",
    "`speakers_of_interest = \"name1,name2,...,nameN\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_of_interest = \"A,B,C,D\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the interviewers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if user specified the interviewer's names then take that\n",
    "if len(speakers_of_interest) > 0:\n",
    "    speakers_of_interest = speakers_of_interest.split(',')\n",
    "    # remove spaces\n",
    "    speakers_of_interest = [x.strip() for x in speakers_of_interest]\n",
    "else:\n",
    "    # else use the first character of the transcription\n",
    "    speakers_of_interest = [corpus[1][0]]\n",
    "    # filter out empty names\n",
    "    speakers_of_interest = [x for x in speakers_of_interest if x != '']\n",
    "    speakers_of_interest = [speakers_of_interest[0]]\n",
    "\n",
    "print(f\"The selected speakers of interest are: {', '.join(speakers_of_interest)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all speakers\n",
    "\n",
    "This part looks for all the names present in the file and checks the independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get interviewer/interviewees names\n",
    "all_speakers = [get_name(x, name_regex) for x in corpus]\n",
    "all_speakers = set(all_speakers)\n",
    "# filter out empty all_speakers\n",
    "all_speakers = [x for x in all_speakers if x != '']\n",
    "\n",
    "# get per file speakers\n",
    "all_speakers_dict = {k: set([get_name(x, name_regex) for x in v]) for k, v in corpus_dict.items()}\n",
    "all_speakers_dict = {k: sorted(v) for k, v in all_speakers_dict.items() if len(v) > 0}\n",
    "\n",
    "\n",
    "# notify user about names\n",
    "print(f\"I found the following speakers names: {', '.join(all_speakers)}\")\n",
    "\n",
    "# notify some of the speakers are not in the list of all speakers\n",
    "if len(speakers_of_interest) > 0:\n",
    "    not_in_list = set(speakers_of_interest) - set(all_speakers)\n",
    "    if len(not_in_list) > 0:\n",
    "        print(f\"Warning: the following speakers are not in the list of all speakers: {', '.join(not_in_list)}\")\n",
    "\n",
    "# build the variable dict\n",
    "dependent_variable_dict = {}\n",
    "variable_files = [json.loads(f) for f in variable_files]\n",
    "\n",
    "# try to guess which one is the independent variable based on presence of speaker names\n",
    "\n",
    "independent_variable_dict = [x for x in variable_files if any([y in all_speakers for y in x.keys()])]\n",
    "if len(independent_variable_dict) == 1:\n",
    "    independent_variable_dict = independent_variable_dict[0]\n",
    "    dependent_variable_dict = [x for x in variable_files if x != independent_variable_dict][0]\n",
    "elif len(independent_variable_dict) > 1:\n",
    "    print(\"Warning: I have found more than one file with independent variables. I will take the first one\")\n",
    "    independent_variable_dict = independent_variable_dict[0]\n",
    "    dependent_variable_dict = [x for x in variable_files if x != independent_variable_dict][0]\n",
    "else:\n",
    "    print(\"Warning: I have not found any file with independent variables. I will take the second one\")\n",
    "    independent_variable_dict = variable_files[1]\n",
    "    dependent_variable_dict = variable_files[0]\n",
    "\n",
    "print(\"Correctly loaded all the variables. Check them out to see if there are any errors\")\n",
    "print(\"Independent variables:\")\n",
    "print(json.dumps(independent_variable_dict, sort_keys=True, indent=4))\n",
    "print(\"Dependent variables:\")\n",
    "print(json.dumps(dependent_variable_dict, sort_keys=True, indent=4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# check the difference between the names in independent variables and the speakers of interest\n",
    "\n",
    "ind_s= set(independent_variable_dict.keys())\n",
    "soi= set(speakers_of_interest)\n",
    "\n",
    "diff_ind = ind_s - soi\n",
    "diff_soi = soi - ind_s\n",
    "\n",
    "if len(diff_soi) > 0:\n",
    "    print(f\"Warning: the following speakers are not in the list of independent variables: {', '.join(diff_soi)}\")\n",
    "if len(diff_ind) > 0:\n",
    "    print(f\"Warning: the following speakers are not in the list of speakers of interest: {', '.join(diff_ind)}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# merge the two dictionaries into a new one and warn on duplicates\n",
    "# order the keys so that the dependent variables are first\n",
    "variable_dict = {**independent_variable_dict, **dependent_variable_dict}\n",
    "if len(variable_dict) != len(independent_variable_dict) + len(dependent_variable_dict):\n",
    "    print(\"Warning: I have found duplicate variables. I will take the first one\")\n",
    "    # print the duplicates\n",
    "    print(\"Duplicates are:\")\n",
    "    for k, v in variable_dict.items():\n",
    "        if k in independent_variable_dict and k in dependent_variable_dict:\n",
    "            print(f\"{k}: {v}\")\n",
    "\n",
    "# get an inverse of the dependent variable\n",
    "idv = {}\n",
    "for k, v in variable_dict.items():\n",
    "    if isinstance(v, list):\n",
    "        for i in v:\n",
    "            idv[i] = k\n",
    "    else:\n",
    "        idv[v] = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output file settings\n",
    "Now it's time to create the output files.\n",
    "\n",
    "There are different outputs and here we specify their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# get the name of the output path\n",
    "output_dir = \"postprocessed\"\n",
    "# create the output directory if it does not exist\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "# define the output file names\n",
    "dataset_path = os.path.join(output_dir, \"dataset.csv\")\n",
    "binary_dataset_path = os.path.join(output_dir, \"binary_dataset.csv\")\n",
    "annotation_info_path = os.path.join(output_dir, \"annotation_info.csv\")\n",
    "not_annotated_path = os.path.join(output_dir, \"missed_annotations.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to add the previous paragraph in the final csv file. For example, when examining an annotation, you want to know what the previous speaker said before the current one (turn taking). If you are interested in this information being in the final output set `previous_line` to `True`, else leave it `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_line = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we include a context for all the annotated tokens present in your data. The context is made of the previous/next words after the annotated one, following a n-gram rule as you can see in the rule below: \n",
    "\n",
    "`ngram_params = (number previous words, number next words)`\n",
    "\n",
    "Here we decided to take into account 10 words appearing before the annotated one and 5 after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_params = (10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a csv file and it needs a header. For this reason, here we define the header with the following elements:\n",
    "- The `token` for the annotated words\n",
    "- The complete list of your variables\n",
    "- (Optional) the previous line\n",
    "- The speaker's name\n",
    "- The context in which the token was found\n",
    "- An `unk` (unknown) category for variables that were found in the annotations but not present in the variable files (useful to catch some errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile regex to find features\n",
    "csv_header = sorted(dependent_variable_dict.keys())\n",
    "\n",
    "# define the end of the csv\n",
    "csv_end = [\"speaker\", \"interlocutor/s\", \"file\", 'context', 'unk']\n",
    "if previous_line:\n",
    "    csv_end.insert(0, 'previous line')\n",
    "csv_header = [\"token\"] + csv_header + csv_end\n",
    "csv_file = [csv_header]\n",
    "unk_categories = []\n",
    "\n",
    "print(f\"The csv header looks like this\")\n",
    "csv_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation information\n",
    "\n",
    "Here we build our counter and logger for the annotations. We want to count the frequency of each annotated token in the corpus, how many times it appears and who uses it. Moreover, for logging reasons, we want to present the tokens which are annotated but were found outside the annotation rule (not recognized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### Finding all the annotated words\n",
    "whole_corpus = \"\\n\".join(corpus)\n",
    "\n",
    "annotations = []\n",
    "for pt, crp in corpus_dict.items():\n",
    "\n",
    "    crp= \"\\n\".join(crp)\n",
    "    anns = feat_regex.finditer(crp)\n",
    "\n",
    "    # check correctness of all annotations\n",
    "    anns, _ = check_correct_annotations(anns, crp, pt, verbose=True)\n",
    "\n",
    "    annotations.extend(anns)\n",
    "\n",
    "\n",
    "annotations = [x.split(\".\")[-1] for x in annotations]\n",
    "# remove square brackets\n",
    "annotations = [x.replace(\"[\", \"\").replace(\"]\", \"\") for x in annotations]\n",
    "\n",
    "# count the number of annotations\n",
    "annotation_counter = Counter(annotations)\n",
    "annotation_counter = {k: dict(annotated=v) for k, v in annotation_counter.items()}\n",
    "\n",
    "print(f\"The total number of annotated words is {len(annotation_counter)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we check the number of times the token appears annotated (following the REGEX rule) vs not, we do this for each annotated token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check if there are any annotations not annotated\n",
    "for token, v in tqdm(annotation_counter.items(), desc=\"Checking annotations\"):\n",
    "    # check for annotation repetitions\n",
    "    wild_rep, wild_rep_interest, ann_rep, _ = find_repetitions(whole_corpus, token, feat_regex, name_regex,\n",
    "                                                               speakers_of_interest)\n",
    "    total_rep = wild_rep + ann_rep\n",
    "    not_annotated = total_rep - v['annotated']\n",
    "    annotation_counter[token]['not annotated'] = not_annotated\n",
    "    annotation_counter[token]['not_annotated_interest'] = wild_rep_interest\n",
    "\n",
    "\n",
    "print(f\"The total repetitions of annotated words is {sum([x['annotated'] for x in annotation_counter.values()])}\")\n",
    "print(\n",
    "    f\"The total repetitions of not annotated words is {sum([x['not annotated'] for x in annotation_counter.values()])}\")\n",
    "print(\n",
    "    f\"The total repetitions of not annotated words from speaker of interest is {sum([x['not_annotated_interest'] for x in annotation_counter.values()])}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the previous cell, counts all the tokens in the whole corpus, we probably want to differentiate between speakers.\n",
    "For this reason, here we need to specify where we want to look for the missed tokens.\n",
    "\n",
    "You have the following options:\n",
    "\n",
    "- Choose all the speakers found in your corpus with: `speakers=all_speakers`\n",
    "- Choose only the speakers you are interested in (defined previously):\n",
    "`speakers=speakers_of_interest`\n",
    "- Choose other speakers you are interested in, by manually enumerating them: `speakers= [\"name1\",\"name2\",...,\"nameN\"]`\n",
    "\n",
    "By default, we check for all the speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which speaker to check for annotations, you can uncomment one of the following lines:\n",
    "speakers = all_speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to find those missing annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_annotated_log = {}\n",
    "\n",
    "\n",
    "for path, crp in tqdm(corpus_dict.items(), desc=\"Finding not annotated words\"):\n",
    "\n",
    "    # filter out the speakers of interest\n",
    "    crp = [x for x in crp if get_name(x, name_regex) in speakers]\n",
    "\n",
    "    # join the corpus\n",
    "    crp = \"\\n\".join(crp)\n",
    "    not_annotated_log[path] = {}\n",
    "    # for all the tokens\n",
    "    for token, _ in annotation_counter.items():\n",
    "        # find the annotations\n",
    "        _, _, _, wna = find_repetitions(crp, token, feat_regex, name_regex, speakers_of_interest, check_annotated=False)\n",
    "        if len(wna) > 0:\n",
    "            if token not in not_annotated_log[path]:\n",
    "                not_annotated_log[path][token] = []\n",
    "            not_annotated_log[path][token] += wna\n",
    "\n",
    "\n",
    "print(f\"The total number of not annotated words is {len(not_annotated_log)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# find the number of not annotated words for each speaker\n",
    "for sp in tqdm(speakers, desc=\"Finding speakers for not annotated words\"):\n",
    "    sp_corpus = [c for c in corpus if get_name(c, name_regex) == sp]\n",
    "    sp_corpus = \"\\n\".join(sp_corpus)\n",
    "    for token, v in annotation_counter.items():\n",
    "        # check for annotation repetitions\n",
    "        wild_rep, wild_rep_interest, ann_rep, _ = find_repetitions(sp_corpus, token, feat_regex, name_regex,\n",
    "                                                                   speakers_of_interest)\n",
    "\n",
    "        if sp + ' not annotated' not in annotation_counter[token]:\n",
    "            annotation_counter[token][sp + ' not annotated'] = 0\n",
    "        if sp + ' annotated' not in annotation_counter[token]:\n",
    "            annotation_counter[token][sp + ' annotated'] = 0\n",
    "\n",
    "        annotation_counter[token][sp + ' not annotated'] = wild_rep\n",
    "        annotation_counter[token][sp + ' annotated'] = ann_rep\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# augment annotation_counter with speakers and add total number\n",
    "for token in annotation_counter.keys():\n",
    "    annotation_counter[token]['total'] = annotation_counter[token]['annotated'] + annotation_counter[token][\n",
    "        'not annotated']\n",
    "    for speaker in speakers_of_interest:\n",
    "        annotation_counter[token][speaker + \" annotated\"] = 0\n",
    "        annotation_counter[token][speaker + \" not annotated\"] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the main loop\n",
    "This part starts the main loop. You don't need to change anything here, if you are interested check out the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for every paragraph in the transcript\n",
    "print(f\"Starting the main loop\")\n",
    "for file_path, corpus in tqdm(corpus_dict.items(), desc=\"Files\"):\n",
    "    file_speakers = all_speakers_dict[file_path]\n",
    "\n",
    "    for idx in tqdm(range(len(corpus)),leave=False, desc=\"Paragraphs\"):\n",
    "        c = corpus[idx]\n",
    "        cur_speaker = get_name(c, name_regex)\n",
    "\n",
    "        # get the paragraph without features\n",
    "        if cur_speaker in speakers_of_interest:\n",
    "            sp = corpus[idx - 1]\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        clean_p, wrong_tags = remove_features(c, square_regex)\n",
    "\n",
    "        # get the features\n",
    "        tags = feat_regex.finditer(c)\n",
    "\n",
    "        # for every tags with features in the paragraph\n",
    "        for t in tags:\n",
    "            # get index of result + tag\n",
    "            index = t.start()\n",
    "            org_t = t.group(0)\n",
    "            t = t.group(1)\n",
    "\n",
    "            # skip annotations that are not valid\n",
    "            if t.split(\".\")[-1] not in annotation_counter.keys():\n",
    "                print(f\"\\nSkipping '{t}' because it is not valid\")\n",
    "                continue\n",
    "\n",
    "            # skip any tags that are wrongly formatted\n",
    "            if any([t in wt for wt in wrong_tags]):\n",
    "                print(f\"\\nSkipping '{t}' because it is wrongly formatted\")\n",
    "                continue\n",
    "\n",
    "            # initialize empty row\n",
    "            csv_line = [\"\" for _ in range(len(csv_header))]\n",
    "\n",
    "            # get independent variable information\n",
    "            for k, v in independent_variable_dict.items():\n",
    "                if cur_speaker != k:\n",
    "                    continue\n",
    "                for var in v:\n",
    "                    category = idv[var]\n",
    "                    cat_idx = csv_header.index(category)\n",
    "                    csv_line[cat_idx] = var\n",
    "\n",
    "            # get the features\n",
    "            feats = t.rsplit(\".\", 1)\n",
    "            text = feats[1]\n",
    "            feats = feats[0]\n",
    "\n",
    "            context = get_ngram(c, ngram_params, index, square_regex)\n",
    "\n",
    "\n",
    "            # for every feature in the word\n",
    "            for f in feats.split(\".\"):\n",
    "                # if the category is not present in the dict, then add to unk\n",
    "                if f not in idv.keys():\n",
    "                    unk_categories.append(f)\n",
    "                    csv_line[-1] = csv_line[-1] + f + \",\"\n",
    "                else:\n",
    "                    category = idv[f]\n",
    "                    cat_idx = csv_header.index(category)\n",
    "                    csv_line[cat_idx] = f\n",
    "\n",
    "            # add initial infos and final unk to the line\n",
    "            # [\"speaker\", \"interlocutor/s\", \"file\", 'context', 'unk']\n",
    "\n",
    "            speakers=copy(file_speakers)\n",
    "            speakers.remove(cur_speaker)\n",
    "\n",
    "            csv_line[0] = text\n",
    "            csv_line[-2] = context\n",
    "            csv_line[-3] = file_path\n",
    "            csv_line[-4] = speakers\n",
    "            csv_line[-5] = cur_speaker\n",
    "            if previous_line:\n",
    "                csv_line[-6] = sp\n",
    "\n",
    "            csv_line[-1] = csv_line[-1].strip(\",\")\n",
    "            csv_file.append(csv_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the output\n",
    "Finally, we need to save the output in the csv file for all our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# write the csv\n",
    "with open(dataset_path, \"w\", newline=\"\", encoding=\"utf16\") as f:\n",
    "    writer = csv.writer(f, delimiter=separator)\n",
    "    writer.writerows(csv_file)\n",
    "\n",
    "# generate the annotation info file\n",
    "header = [\"token\"] + list(list(annotation_counter.values())[0].keys())\n",
    "\n",
    "with open(annotation_info_path, \"w\", newline=\"\", encoding=\"utf16\") as f:\n",
    "    writer = csv.writer(f, delimiter=separator)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for k, v in annotation_counter.items():\n",
    "        writer.writerow([k] + list(v.values()))\n",
    "\n",
    "# save the not annotated log\n",
    "if len(not_annotated_log) > 0:\n",
    "    header = [\"token\"]\n",
    "\n",
    "    # count the maximum length for all the values of the values of the dict\n",
    "    max_lens = max([len(x) for sub in not_annotated_log.values() for x in sub.values()])\n",
    "    header += [f\"context {i}\" for i in range(1, max_lens + 1)]\n",
    "\n",
    "    with open(not_annotated_path, \"w\", newline=\"\", encoding=\"utf16\") as f:\n",
    "        writer = csv.writer(f, delimiter=separator)\n",
    "        writer.writerow(header)\n",
    "        for path, vals in not_annotated_log.items():\n",
    "            for token, context in vals.items():\n",
    "                writer.writerow([path, token] + context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to download it right away run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download(dataset_path)\n",
    "files.download(annotation_info_path)\n",
    "if len(not_annotated_log) > 0:\n",
    "    files.download(not_annotated_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary Dataset\n",
    "The original dataset likely had categorical variables with multiple possible values represented as text or numbers. In order to perform certain types of analysis or feed the data into a machine learning model, it's often helpful to convert these categorical variables into a numerical format. One way to do this is through one-hot encoding, where a new binary column is created for each possible value of a categorical variable. This new dataset will differ from the original in that it will have more columns, one for each possible value of the categorical variables. Additionally, each row will now contain only 0's and 1's. The benefit of having the dataset encoded in this format is that it allows for the data to be easily processed by many machine learning algorithms, since they often expect numerical data as input. Additionally, one-hot encoding can help improve the performance of certain types of models, such as decision trees, by allowing them to make splits on categorical variables without having to convert them to numerical values first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the binary dataset file\n",
    "# read the csv file to pandas dataframe\n",
    "df = pd.read_csv(dataset_path, sep=separator, encoding=\"utf16\")\n",
    "to_drop = [\"context\", \"token\",\"unk\",\"file\"]\n",
    "\n",
    "tokens = df[\"token\"]\n",
    "context = df['context']\n",
    "\n",
    "# drop first and last two columns\n",
    "df = df.drop(to_drop, axis=1)\n",
    "df_encoded = pd.get_dummies(df, columns=df.columns, prefix_sep=\":\")\n",
    "df_encoded[\"token\"] = tokens\n",
    "df_encoded[\"context\"] = context\n",
    "\n",
    "df_encoded.to_csv(binary_dataset_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally to download it run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download(binary_dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown categories\n",
    "Here, we show the unknown category, if any could be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(unk_categories) > 0:\n",
    "    unk_categories = set(unk_categories)\n",
    "    unk_categories = sorted(unk_categories)\n",
    "    print(print(\n",
    "        f\"I have found several categories not listed in your variable file.\\n\"\n",
    "        f\"Following in alphabetical order:\"))\n",
    "    for idx, c in enumerate(unk_categories):\n",
    "        print(idx, f\"'{c}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
