{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Download the code in colab"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/nicofirst1/TranscriptionTagger"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import necessary libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcopy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m copy\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolab\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m files\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "from copy import copy\n",
    "from google.colab import files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom files and settings\n",
    "\n",
    "In this section we will define some settings for your transcription!\n",
    "We will start with the variables you defined and move on to how to find them in your annotated corpus (REGEX).\n",
    "\n",
    "## Variables"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining variables with JSON\n",
    "When annotating your data, you will for sure use some variables. These variables may have a hierarchical structure, where one category includes many variations. To allow the program to find your variables you have to build a \"dictionary\" where you specify them.\n",
    "Here, we use JSON files that allow you to come up with how many categories and variables you want in a clear and defined manner. Check out the [introduction to JSON tutorial](https://www.w3schools.com/js/js_json_intro.asp) if you are not familiar with it.\n",
    "You can also look at the  [dependent variables](./dependent_variables.json) we are using in this project.\n",
    "\n",
    "### Your files\n",
    "Now that you are familiar with how the JSON syntax work, we need to define what your variables are. Here you have two options:\n",
    "\n",
    "#### 1. Use the default variables file\n",
    "At the moment there are two files, one for [dependent variables](./dependent_variables.json) and one for [independent variables](./independent_variables.json) in this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_variable_path = 'TranscriptionTagger/dependent_variables.json'\n",
    "independent_variable_path = 'TranscriptionTagger/independent_variables.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regular Expressions (REGEX)\n",
    "\n",
    "Following, the regular expressions (REGEX) to find your annotation in the corpus.\n",
    "\n",
    "To come up with a new tag REGEX you can use [regex101](https://regex101.com/). To check out how it works, open  [regex101](https://regex101.com/). Copy-paste the\n",
    "content of `square_regex` (`(\\[\\$[\\S ]*?\\])`) into the regular expression bar (on the top) and a sample paragraph in the test string (on the bottom), e.g.:\n",
    "```\n",
    "S    akiid, akiid, bi l [$G-OTH.fooxinende], aa, yaʕni il jumʕa la bass ʕidna sabit uu aħħad iħna [$DEM-HAAY.haay] [$G-OTH.daayrakt] leen il alwaad ʕidna       aa, iða j jaww [$IA.kulliʃ] [$IA.zeen] insawwi maʃaawi, w iða j jaww mu [$IA.zeen], insawwi yaʕni l aklaat illi tijmaʕ il ʕaaʔila, tiðakkiriin iħna l ʕiraaqiyiin         id dooLMa, w is [$CK.simaC], (laughing), w il lamma l ħilwa w il aħfaad, fa insawwi [$IA.CK.hiiCi]          bass il yoom la yaʕni innu aani w il ħajji [$GQ.gaaʕdiin], akθar il marraat nugʕud iS SuBiħ [$IA.nitrayyag], baʕdeen il, il gahwa uu baʕdeen nuqʕud insoolif, inʃuuf [$IA.ʃinu] ʕidna maʃaariiʕ, niTLaʕ maθalan irruuħ nimʃi [$IA.fadd] niSS saaʕa saaʕa\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "square_regex = re.compile(r\"(\\[\\$[\\S ]*?\\])\")\n",
    "feat_regex = re.compile(r'\\[\\$([\\S ]*?)\\]')\n",
    "sequence_regex = re.compile(r\"({[\\S ]+})\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to upload your transcription file. Be sure to upload a `.txt` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_path = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "separator for the csv file<br>\n",
    "Alternatives are:<br>\n",
    "semicolon: ';'<br>\n",
    "comma: ','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = '\\t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(corpus):\n",
    "    \"\"\"\n",
    "    Remove the features from the corpus\n",
    "    \"\"\"\n",
    "    corpus = copy(corpus)\n",
    "    words = square_regex.findall(corpus)\n",
    "    for w in words:\n",
    "        try:\n",
    "            text = w.rsplit(\".\", 1)[1][:-1]\n",
    "            corpus = corpus.replace(w, text)\n",
    "        except IndexError:\n",
    "            print(f\"I found an error for the tag '{w}'. Myabe it does not have a point in it?\\n\"\n",
    "                        f\"Please check the tag and try again.\", \"error\")\n",
    "            exit()\n",
    "            continue\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(line):\n",
    "    return line.split(\" \")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(transcription_path):\n",
    "    # Load the dependent variable\n",
    "    with open(dependent_variable_path, 'r') as f:\n",
    "        dependent_variable = json.load(f)\n",
    "\n",
    "    # Load the dependent variable\n",
    "    with open(independent_variable_path, 'r') as f:\n",
    "        independent_variable = json.load(f)\n",
    "    dependent_variable.update(independent_variable)\n",
    "    # get an inverse of the dependent variable\n",
    "    idv = {}\n",
    "    for k, v in dependent_variable.items():\n",
    "        if isinstance(v, list):\n",
    "            for i in v:\n",
    "                idv[i] = k\n",
    "        else:\n",
    "            idv[v] = k\n",
    "\n",
    "    # ask user for path input\n",
    "    path = input(print(f'Drag and drop the transcription file (.txt), or leave blank if \"{transcription_path}\" '\n",
    "                      f'is correct: '))\n",
    "    print(\"\\n\")\n",
    "    if len(path) > 0:\n",
    "        transcription_path = path.strip()\n",
    "    output_path = transcription_path.replace('.txt', '_output.csv')\n",
    "\n",
    "    # opend the file\n",
    "    with open(transcription_path, 'r+', encoding=\"utf16\") as f:\n",
    "        trans = f.readlines()\n",
    "    trans = trans[2:]\n",
    "    trans = [x.strip() for x in trans]\n",
    "    trans = [x for x in trans if x != '']\n",
    "\n",
    "    # ask for interviwers name\n",
    "    interviewers = input(print(f'Add the name(s) of the interviewer(s) (separated by comma), '\n",
    "                              f'leave empty for classical interviewer-interviewees structure: '))\n",
    "    print(\"\\n\")\n",
    "    if len(interviewers) > 0:\n",
    "        interviewers = interviewers.split(',')\n",
    "    else:\n",
    "        interviewers = trans[0][0]\n",
    "\n",
    "    # ask for previous line\n",
    "    previous_line = input(print(f'When generating the final cvs file, I can also include the speaker utterance.'\n",
    "                               f' Do you want me to include it? (y/n): '))\n",
    "    print(\"\\n\")\n",
    "    if previous_line == 'y':\n",
    "        previous_line = True\n",
    "    else:\n",
    "        previous_line = False\n",
    "\n",
    "    # get speak/list names\n",
    "    names = [get_name(x).strip() for x in trans]\n",
    "    names = set(names)\n",
    "\n",
    "    # remove all mention of interviwers in names\n",
    "    for i in interviewers:\n",
    "        names = [x for x in names if i not in x]\n",
    "    interviewees = list(names)\n",
    "\n",
    "    # notify user about names\n",
    "    print(print(f\"I found the following names: {', '.join(interviewees)}\"))\n",
    "\n",
    "    # compile regex to find features\n",
    "    csv_header = list(dependent_variable.keys())\n",
    "\n",
    "    # define the end of the csv\n",
    "    csv_end = ['sequence in sentence', 'unk']\n",
    "    if previous_line:\n",
    "        csv_end.insert(0, 'previous line')\n",
    "    csv_header = [\"text\"] + csv_header + csv_end\n",
    "    csv_file = [csv_header]\n",
    "    unk_categories = []\n",
    "\n",
    "    # for every paragraph in the transcript\n",
    "    for idx in range(len(trans)):\n",
    "        c = trans[idx]\n",
    "\n",
    "        # get the paragraph without features\n",
    "        if get_name(c) in interviewees:\n",
    "            sp = trans[idx - 1]\n",
    "        else:\n",
    "            continue\n",
    "        clean_p = remove_features(c)\n",
    "\n",
    "        # capture all the sequences\n",
    "        sequences = sequence_regex.finditer(clean_p)\n",
    "        sequences = [(x.start(), x.end(), x.group()) for x in sequences]\n",
    "\n",
    "        # get the features\n",
    "        tags = feat_regex.finditer(c)\n",
    "\n",
    "        # for every tags with features in the paragraph\n",
    "        for t in tags:\n",
    "            # get index of result + tag\n",
    "            index = t.start()\n",
    "            t = t.group(1)\n",
    "\n",
    "            # initialize empty row\n",
    "            csv_line = [\"\" for _ in range(len(csv_header))]\n",
    "\n",
    "            # get the features\n",
    "            feats = t.rsplit(\".\", 1)\n",
    "            text = feats[1]\n",
    "            feats = feats[0]\n",
    "\n",
    "            # for every feature in the word\n",
    "            for f in feats.split(\".\"):\n",
    "                # if the category is not present in the dict, then add to unk\n",
    "                if f not in idv.keys():\n",
    "                    unk_categories.append(f)\n",
    "                    csv_line[-1] = csv_line[-1] + f + \",\"\n",
    "                else:\n",
    "                    category = idv[f]\n",
    "                    cat_idx = csv_header.index(category)\n",
    "                    csv_line[cat_idx] = f\n",
    "\n",
    "            # add initial infos and final unk to the line\n",
    "            csv_line[0] = text\n",
    "            if previous_line:\n",
    "                csv_line[-3] = sp\n",
    "\n",
    "            # add the sequence to the line\n",
    "            if len(sequences) != 0:\n",
    "                for s in sequences:\n",
    "                    seq_start, seq_end, seq = sequences[0]\n",
    "                    if seq_start < index < seq_end:\n",
    "                        seq = seq.replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "                        csv_line[-2] = seq\n",
    "            csv_line[-1] = csv_line[-1].strip(\",\")\n",
    "            csv_file.append(csv_line)\n",
    "\n",
    "    # write the csv\n",
    "    with open(output_path, \"w\", newline=\"\", encoding=\"utf16\") as f:\n",
    "        writer = csv.writer(f, delimiter=separator)\n",
    "        writer.writerows(csv_file)\n",
    "    print(f\"Done!\\nFile has been saved in '{output_path}'\",\"ok\")\n",
    "    if len(unk_categories) > 0:\n",
    "        unk_categories = set(unk_categories)\n",
    "        unk_categories = sorted(unk_categories)\n",
    "        print(print(\n",
    "            f\"I have found several categories not listed in '{independent_variable_path}' or in '{dependent_variable_path}'.\\n\"\n",
    "            f\"Following in alphabetical order:\"))\n",
    "        for c in unk_categories:\n",
    "            print(print(c.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        main(transcription_path)\n",
    "    except Exception as e:\n",
    "        print(print(f\"An error occured: {e}\"))\n",
    "        exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
