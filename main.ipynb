{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'TranscriptionTagger'...\r\n",
      "remote: Enumerating objects: 56, done.\u001B[K\r\n",
      "remote: Counting objects: 100% (56/56), done.\u001B[K\r\n",
      "remote: Compressing objects: 100% (52/52), done.\u001B[K\r\n",
      "remote: Total 56 (delta 30), reused 26 (delta 0), pack-reused 0\u001B[K\r\n",
      "Receiving objects: 100% (56/56), 38.21 KiB | 1.66 MiB/s, done.\r\n",
      "Resolving deltas: 100% (30/30), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/nicofirst1/TranscriptionTagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[61], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcopy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m copy\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mos\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolab\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m files\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "from copy import copy\n",
    "import os\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom files and settings\n",
    "\n",
    "In this section we will define some settings for your transcription!\n",
    "We will start with the variables you defined and move on to how to find them in your annotated corpus (REGEX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "### Defining variables with JSON\n",
    "When annotating your data, you will for sure use some variables. These variables may have a hierarchical structure, where one category includes many variations. To allow the program to find your variables you have to build a \"dictionary\" where you specify them.\n",
    "Here, we use JSON files that allow you to come up with how many categories and variables you want in a clear and defined manner. Check out the [introduction to JSON tutorial](https://www.w3schools.com/js/js_json_intro.asp) if you are not familiar with it.\n",
    "You can also look at the  [dependent variables](./dependent_variables.json) we are using in this project.\n",
    "\n",
    "### Your files\n",
    "Now that you are familiar with how the JSON syntax work, we need to define what your variables are. Here you have two options, depending on the option you choose you will need to run different cells.\n",
    "\n",
    "#### 1. Use the default variables file\n",
    "At the moment there are two files, one for [dependent variables](./dependent_variables.json) and one for [independent variables](./independent_variables.json) in this code. If you open these link you will see the original files (not modificable), but you can open them from this web page on the top right (check out [this tutorial](https://neptune.ai/blog/google-colab-dealing-with-files) on how to access local files system to google colab, point 4), you can find the files inside the `TranscriptionTagger` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['{\\n  \"Typ IA\": \"IA\",\\n  \"Oth L\" : \"OTH-L\",\\n  \"SA\":  \"OTH-SA\",\\n  \"German\" : [\"G-SCHOOL\", \"G-JOB\", \"G-FRIE\", \"G-GER\", \"G-COV\", \"G-OTH\"],\\n  \"Relig Phrase\": \"RELIG\",\\n  \"Oth Variety\": \"OTH-DIAL\",\\n  \"Demonstratives\": [\"DEM-HA-BEG\", \"DEM-A-END\", \"DEM-NO-HA-BEG\", \"DEM-NO-A-END\", \"DEM-HAL\", \"DEM-HAAY\", \"DEM-HAAYA\", \"DEM-HA\", \"DEM-I-END\", \"DEM-DHOOLE\"],\\n  \"maal\": [\"MAAL\", \"NO-MAAL\"],\\n  \"mu\": \"MU\",\\n  \"Präfix\" : [\"DA\", \"JAAY\", \"GAAM\"],\\n  \"Suffix\": [\"NO-N\", \"SUF-IINA\", \"SUF-NO-IINA\"],\\n  \"K/Č\": [\"KC\", \"CK\"],\\n  \"Q/G\": [\"QG\", \"GQ\"],\\n  \"Q/K\" : [\"QK\", \"KQ\"],\\n  \"Syllable Structure\": [\"SS-DIF\", \"SS-LEV\"]\\n\\n}',\n '{\\n  \"Something\": \"something else\"\\n\\n}']"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependent_variable_path = 'TranscriptionTagger/dependent_variables.json'\n",
    "independent_variable_path = 'TranscriptionTagger/independent_variables.json'\n",
    "\n",
    "variable_files=[dependent_variable_path, independent_variable_path]\n",
    "variable_files=[open(path, \"r+\") for path in variable_files]\n",
    "variable_files=[x.read() for x in variable_files]\n",
    "variable_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Upload your own files\n",
    "Here you can upload your own variable files as long as they are still in JSON format. You can upload how many files you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_files=files.upload()\n",
    "\n",
    "for fn in variable_files.keys():\n",
    "    if \".json\" not in fn:\n",
    "        raise FileNotFoundError(f\"File {fn} is not a JSON file!\")\n",
    "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(variable_files[fn])))\n",
    "\n",
    "variable_files=list(variable_files.values())\n",
    "\n",
    "# decode\n",
    "variable_files=[x.decode(\"utf8\") for x in variable_files]\n",
    "variable_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions (REGEX)\n",
    "\n",
    "Now it's time to define the regular regular expressions (REGEX) to find your annotation in the corpus.\n",
    "\n",
    "In our project each annotation is contained in square brackets and starts with a dollar sign. The variables are divided by a dot (without spaces) and the last element after the dot is the annotated word (with spaces).\n",
    "For example : `[$variable1.variable2.annotated word]`\n",
    "\n",
    "To come up with a new tag REGEX you can use [regex101](https://regex101.com/). To check out how it works, open  [regex101](https://regex101.com/). Copy-paste the\n",
    "content of `square_regex` (`(\\[\\$[\\S ]*?\\])`) into the regular expression bar (on the top) and a sample paragraph in the test string (on the bottom), e.g.:\n",
    "```\n",
    "S    akiid, akiid, bi l [$G-OTH.fooxinende], aa, yaʕni il jumʕa la bass ʕidna sabit uu aħħad iħna [$DEM-HAAY.haay] [$G-OTH.daayrakt] leen il alwaad ʕidna       aa, iða j jaww [$IA.kulliʃ] [$IA.zeen] insawwi maʃaawi, w iða j jaww mu [$IA.zeen], insawwi yaʕni l aklaat illi tijmaʕ il ʕaaʔila, tiðakkiriin iħna l ʕiraaqiyiin         id dooLMa, w is [$CK.simaC], (laughing), w il lamma l ħilwa w il aħfaad, fa insawwi [$IA.CK.hiiCi]          bass il yoom la yaʕni innu aani w il ħajji [$GQ.gaaʕdiin], akθar il marraat nugʕud iS SuBiħ [$IA.nitrayyag], baʕdeen il, il gahwa uu baʕdeen nuqʕud insoolif, inʃuuf [$IA.ʃinu] ʕidna maʃaariiʕ, niTLaʕ maθalan irruuħ nimʃi [$IA.fadd] niSS saaʕa saaʕa\n",
    "```\n",
    "\n",
    "Define the REGEXs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_regex = re.compile(r\"(\\[\\$[\\S ]*?\\])\")\n",
    "feat_regex = re.compile(r'\\[\\$([\\S ]*?)\\]')\n",
    "sequence_regex = re.compile(r\"({[\\S ]+})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcription file\n",
    "Finally, you need to upload your transcription file. This is the filed containing your annotated corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# DONT USE THIS, IT IS JUST FOR DEBUG\n",
    "\n",
    "if True:\n",
    "    transcription_path=\"/home/dizzi/Downloads/Aya.txt\"\n",
    "    with open(transcription_path, \"r+\", encoding=\"utf16\") as f:\n",
    "        transcription_text=f.read()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_path = files.upload()\n",
    "\n",
    "assert len(transcription_path) ==1 , \"Support for multiple transcription file is not available! Please upload just one file\"\n",
    "transcription_text=list(transcription_path.values())[0]\n",
    "transcription_text=transcription_text.decode(\"utf16\")\n",
    "\n",
    "# print first 50 character of the transcription file\n",
    "print(\"\\n\\n First 400 characters:\\n\")\n",
    "transcription_text[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output file\n",
    "\n",
    "In order to obtain the comma separated values (`csv`) file, you can specify the separator. By default the separator value is a comma (as the name implies), but you can also use:\n",
    "- semicolon: `;`\n",
    "- comma: `,`\n",
    "- tab : `\\t`\n",
    "\n",
    "In our project we use the tab, since it is the one that allows excel to view it (for Germany the default value is tab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = '\\t'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "Following some functions that we will use later in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(corpus):\n",
    "    \"\"\"\n",
    "    Remove the features from the corpus\n",
    "    \"\"\"\n",
    "    corpus = copy(corpus)\n",
    "    words = square_regex.findall(corpus)\n",
    "    for w in words:\n",
    "        try:\n",
    "            text = w.rsplit(\".\", 1)[1][:-1]\n",
    "            corpus = corpus.replace(w, text)\n",
    "        except IndexError:\n",
    "            print(f\"I found an error for the tag '{w}'. Myabe it does not have a point in it?\\n\"\n",
    "                        f\"Please check the tag and try again.\", \"error\")\n",
    "            exit()\n",
    "            continue\n",
    "    return corpus\n",
    "\n",
    "def get_name(line):\n",
    "    return line.split(\" \")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program\n",
    "Time to start the main program. First let us merge all the variables we uploaded before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correctly loaded all the variables. Check them out to see if there are any errors\n",
      "{\n",
      "    \"Demonstratives\": [\n",
      "        \"DEM-HA-BEG\",\n",
      "        \"DEM-A-END\",\n",
      "        \"DEM-NO-HA-BEG\",\n",
      "        \"DEM-NO-A-END\",\n",
      "        \"DEM-HAL\",\n",
      "        \"DEM-HAAY\",\n",
      "        \"DEM-HAAYA\",\n",
      "        \"DEM-HA\",\n",
      "        \"DEM-I-END\",\n",
      "        \"DEM-DHOOLE\"\n",
      "    ],\n",
      "    \"German\": [\n",
      "        \"G-SCHOOL\",\n",
      "        \"G-JOB\",\n",
      "        \"G-FRIE\",\n",
      "        \"G-GER\",\n",
      "        \"G-COV\",\n",
      "        \"G-OTH\"\n",
      "    ],\n",
      "    \"K/\\u010c\": [\n",
      "        \"KC\",\n",
      "        \"CK\"\n",
      "    ],\n",
      "    \"Oth L\": \"OTH-L\",\n",
      "    \"Oth Variety\": \"OTH-DIAL\",\n",
      "    \"Pr\\u00e4fix\": [\n",
      "        \"DA\",\n",
      "        \"JAAY\",\n",
      "        \"GAAM\"\n",
      "    ],\n",
      "    \"Q/G\": [\n",
      "        \"QG\",\n",
      "        \"GQ\"\n",
      "    ],\n",
      "    \"Q/K\": [\n",
      "        \"QK\",\n",
      "        \"KQ\"\n",
      "    ],\n",
      "    \"Relig Phrase\": \"RELIG\",\n",
      "    \"SA\": \"OTH-SA\",\n",
      "    \"Something\": \"something else\",\n",
      "    \"Suffix\": [\n",
      "        \"NO-N\",\n",
      "        \"SUF-IINA\",\n",
      "        \"SUF-NO-IINA\"\n",
      "    ],\n",
      "    \"Syllable Structure\": [\n",
      "        \"SS-DIF\",\n",
      "        \"SS-LEV\"\n",
      "    ],\n",
      "    \"Typ IA\": \"IA\",\n",
      "    \"maal\": [\n",
      "        \"MAAL\",\n",
      "        \"NO-MAAL\"\n",
      "    ],\n",
      "    \"mu\": \"MU\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "variable_files=[json.loads(f) for f in variable_files]\n",
    "\n",
    "variable_dict = {}\n",
    "\n",
    "for j in variable_files:\n",
    "    for k,v in j.items():\n",
    "        if k in variable_dict.keys():\n",
    "            print(f\"Warning : I have found a duplicate variable named '{k}' with values '{v}'. \"\n",
    "                  f\"If this is expected ignore this message, otherwise check the variables files for duplicates!\")\n",
    "        variable_dict[k]=v\n",
    "\n",
    "\n",
    "\n",
    "# get an inverse of the dependent variable\n",
    "idv = {}\n",
    "for k, v in variable_dict.items():\n",
    "    if isinstance(v, list):\n",
    "        for i in v:\n",
    "            idv[i] = k\n",
    "    else:\n",
    "        idv[v] = k\n",
    "\n",
    "print(\"Correctly loaded all the variables. Check them out to see if there are any errors\")\n",
    "print(json.dumps(variable_dict,sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing transcriptions\n",
    "Based on your type of annotated corpus (aka transcription) you will need to preprocess the file. Here we have 4 steps.\n",
    "Feel free to comment out (add `#` at the start of the line) any preprocessing that does not fit your criteria.\n",
    "To give you a sense of our transcriptions here is how the first 5 paragraphs look like:\n",
    "\n",
    "```\n",
    "file:///aishug294879ryshfda9763afo8947a5gf\n",
    "2022 Oct 21, Fri 10:30\n",
    "\n",
    "B       marħaba ʕeeni ʃ axbaariC, iħna niʃakkariC ihwaaya lennahu, ey    fitaħtuulna beetkum uu istaqbaltuuna             hm, iħCiilna l yoom iʃ sawweeti, ʃinu Caan maʃruuʕiC aSLan, (laughing)     hm  ey  hm  ii bi l ʕaafya      ey  ahh  (laughing)  Caan huwwa imxaTTiT innu tijiin inti haaða l isbuuʕ\n",
    "S           halow ħabiibti, halow ʕeeni, il ħam..., (...) baSiiTa itdallili uu haaða abSaT ʃii insawwiilkumiyaa            yaa miyyat hala biikum    il yoom? il yoom ma ʕindi ʃii, aa, mit... mittafqiin ʕal [$MAAL.mawʕid maalatkum] il yoom, fa [$SS-DIF.gaʕadt] iS Subiħ, aa sawweet [$OTH-SA.SS-DIF.ifTuur] aani w il ħajji, rayyagta ab... aLLa [$CK.yʕaafiiC] uu baʕdeen ijeet ʕala mawʕidna [??ihnaana], [$GQ.gaaʕda] antiDurkum, SaLLeet, ma ʕindi ʃii baʕad, da antiDurkum, (laughing)      ey liʔannahu mittafqiin ʕa l mawʕid fa ma [$GQ.nigdar] inɣayyra\n",
    "\n",
    "\n",
    "B   eh, la, yaʕni aa, waħħad ʕan il mawʕid Caan inti yaʕni ʕindiC ɣeer barnaamij maθalan bi l fooxinende        ey  hm  ee, id doo...,(laughing), ii, (laughing), w il lamma l ħilwa, (laughing)   ee  ey          hm  ey  hm, ħeel zeen\n",
    "S    akiid, akiid, bi l [$G-OTH.fooxinende], aa, yaʕni il jumʕa la bass ʕidna sabit uu aħħad iħna [$DEM-HAAY.haay] [$G-OTH.daayrakt] leen il alwaad ʕidna       aa, iða j jaww [$IA.kulliʃ] [$IA.zeen] insawwi maʃaawi, w iða j jaww mu [$IA.zeen], insawwi yaʕni l aklaat illi tijmaʕ il ʕaaʔila, tiðakkiriin iħna l ʕiraaqiyiin         id dooLMa, w is [$CK.simaC], (laughing), w il lamma l ħilwa w il aħfaad, fa insawwi [$IA.CK.hiiCi]          bass il yoom la yaʕni innu aani w il ħajji [$GQ.gaaʕdiin], akθar il marraat nugʕud iS SuBiħ [$IA.nitrayyag], baʕdeen il, il gahwa uu baʕdeen nuqʕud insoolif, inʃuuf [$IA.ʃinu] ʕidna maʃaariiʕ, niTLaʕ maθalan irruuħ nimʃi [$IA.fadd] niSS saaʕa saaʕa\n",
    "\n",
    "```\n",
    "As you can see our file structure contains two lines with the file name and date, then an empty lines and then a repeating structure of the kind:\n",
    "- interviewer name (`B`), tab, paragraph, newline\n",
    "- interviewee name (`S`), tab, paragraph, newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B  haloow aaya, ʃ ʃooniC? zeena ħamd il laa, ʃ axbaariC, (laughing)   furSa... (laughing), huwwa, hammaateen da ariid agulliC innu furSa saʕiida innu tʕarrafit ʕaleeC waLLa, aLLa ysallmiC  hassa iħCiili, ʃ sawweeti ma sawweeti l yoom, qabil ma nijiiC iħna? ah  hm   ahm(laughing)\n",
      "A [haloow $IA.ʃooniC], [$RELIG.ħamd-il-laa] b xeer inti haloow [$IA.ʃooniC]? waLLa [$RELIG.l ħamd il laa], tamaam, furSa saʕiida tʕarrafit [$CK.ʕaleeC], (laughing) aani l asʕad waLLa tʃarrafit [$CK.biiC] il yoom [$QG.qaʕadit] saaʕa tisʕa, aahm, ʃwayya wiyya l ʕaaʔila leen yoom [$GE.wooxenende], ijjammaʕna ʕa rayuug uu baʕdeen dirasit, uu ħadd ma intiDHaritkum tijuun uu baʕad iltiqeet b ħadaratkum, baʕad ma sawweet ʃii liʔann [$CK.baaCir] ʕindi [$G-SCHOOL.teest] fa laazim aħaDDir, uu [$DA.da adrus] ʃwayya ʕindi [$GE.SCHOOL.ʃitres], (laughing)\n",
      "B   ooh, (laughing), maʕnaatha iħna l yoom raaħ niʃuɣLiC ʃwayya, (laughing), raaħ naaxið min waqtiC hm in ʃaa La ya raBBi kull il tawfiiq (...), uu b il ʕaada yaʕni haaða r rootiin yoomi iliC, bi n nisba iliC?   haahm  ay SaʕuB, ahah\n"
     ]
    }
   ],
   "source": [
    "# step 1: split the whole corpus in different elements every new line, creating a list of paragraphs. For us this means splitting interviewer and interviewee in different paragraphs\n",
    "trans= transcription_text.split(\"\\n\")\n",
    "# step 2: our first two paragraphs are the file name and the date, which we don't need so discard them\n",
    "trans = trans[2:]\n",
    "# step 3: remove spaces at the start and end of each paragraph\n",
    "trans = [x.strip() for x in trans]\n",
    "# step 4 : remove empty paragraphs from the list\n",
    "trans = [x for x in trans if x != '']\n",
    "\n",
    "# take a peek at the first four paragraphs\n",
    "for idx in range(3):\n",
    "    print(trans[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interviewers and Interviewees\n",
    "As you can see form the previous example, we consider a file where we have only one interviewer and one interviewee alternating each other. But it can be the case where you have multiple interviewers and interviewees in random order. In this case we need to know the names of the interviewers in order to split them from the interviewees.\n",
    "\n",
    "If you need to add multiple interviewers run the following cell after adding the names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interviewers = \"name1,name2,...,nameN\"\n",
    "interviewers=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the interviewers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The selected interviewers are B\n"
     ]
    }
   ],
   "source": [
    "# if user specified the interviewer's names then take that\n",
    "if len(interviewers) > 0:\n",
    "    interviewers = interviewers.split(',')\n",
    "    # remove spaces\n",
    "    interviewers = [x.strip() for x in interviewers]\n",
    "else:\n",
    "    # else use the first character of the transcription\n",
    "    interviewers = [trans[0][0]]\n",
    "\n",
    "print(f\"The selected interviewers are {', '.join(interviewers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output file settings\n",
    "Now it's time to create the output file.\n",
    "\n",
    "The output file will have the same name as the transcription one but with `_output.csv` at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'Aya_output.csv'"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the name of the output path\n",
    "output_path=os.path.basename(transcription_path)\n",
    "output_path=os.path.splitext(output_path)[0]+\"_output.csv\"\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to add the previous paragraph in the final csv file. For example, when examining an annotation, you want to know what the previous speaker said before the current one. If you are interested in this information being in the final output set `previous_line` to `True`, else leave it `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#previous_line = True\n",
    "previous_line = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a csv file and it needs a header. For this reason herre we define the header as the following elements:\n",
    "- the `text` for the annotated words\n",
    "- the complete list of variables\n",
    "- (Optional) the previous line\n",
    "- The sentence in which the text was found\n",
    "- an `unk` (unkown) category for variables that were found in the annotations but not present in the variable files (useful to catch some errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The csv header looks like this\n"
     ]
    },
    {
     "data": {
      "text/plain": "['text',\n 'Typ IA',\n 'Oth L',\n 'SA',\n 'German',\n 'Relig Phrase',\n 'Oth Variety',\n 'Demonstratives',\n 'maal',\n 'mu',\n 'Präfix',\n 'Suffix',\n 'K/Č',\n 'Q/G',\n 'Q/K',\n 'Syllable Structure',\n 'Something',\n 'sequence in sentence',\n 'unk']"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# compile regex to find features\n",
    "csv_header = list(variable_dict.keys())\n",
    "\n",
    "# define the end of the csv\n",
    "csv_end = ['sequence in sentence', 'unk']\n",
    "if previous_line:\n",
    "    csv_end.insert(0, 'previous line')\n",
    "csv_header = [\"text\"] + csv_header + csv_end\n",
    "csv_file = [csv_header]\n",
    "unk_categories = []\n",
    "\n",
    "print(f\"The csv header looks like this\")\n",
    "csv_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding interviewees\n",
    "\n",
    "This part looks for all the names present in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I found the following interviewees names: A, A(laughing)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get interviewer/interviewees names\n",
    "names = [get_name(x).strip() for x in trans]\n",
    "names = set(names)\n",
    "\n",
    "# remove all mention of interviwers in names\n",
    "for i in interviewers:\n",
    "    names = [x for x in names if i not in x]\n",
    "interviewees = list(names)\n",
    "\n",
    "# notify user about names\n",
    "print(f\"I found the following interviewees names: {', '.join(interviewees)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting the main loop\n",
    "This part starts the main loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for every paragraph in the transcript\n",
    "for idx in range(len(trans)):\n",
    "    c = trans[idx]\n",
    "\n",
    "    # get the paragraph without features\n",
    "    if get_name(c) in interviewees:\n",
    "        sp = trans[idx - 1]\n",
    "    else:\n",
    "        continue\n",
    "    clean_p = remove_features(c)\n",
    "\n",
    "    # capture all the sequences\n",
    "    sequences = sequence_regex.finditer(clean_p)\n",
    "    sequences = [(x.start(), x.end(), x.group()) for x in sequences]\n",
    "\n",
    "    # get the features\n",
    "    tags = feat_regex.finditer(c)\n",
    "\n",
    "    # for every tags with features in the paragraph\n",
    "    for t in tags:\n",
    "        # get index of result + tag\n",
    "        index = t.start()\n",
    "        t = t.group(1)\n",
    "\n",
    "        # initialize empty row\n",
    "        csv_line = [\"\" for _ in range(len(csv_header))]\n",
    "\n",
    "        # get the features\n",
    "        feats = t.rsplit(\".\", 1)\n",
    "        text = feats[1]\n",
    "        feats = feats[0]\n",
    "\n",
    "        # for every feature in the word\n",
    "        for f in feats.split(\".\"):\n",
    "            # if the category is not present in the dict, then add to unk\n",
    "            if f not in idv.keys():\n",
    "                unk_categories.append(f)\n",
    "                csv_line[-1] = csv_line[-1] + f + \",\"\n",
    "            else:\n",
    "                category = idv[f]\n",
    "                cat_idx = csv_header.index(category)\n",
    "                csv_line[cat_idx] = f\n",
    "\n",
    "        # add initial infos and final unk to the line\n",
    "        csv_line[0] = text\n",
    "        if previous_line:\n",
    "            csv_line[-3] = sp\n",
    "\n",
    "        # add the sequence to the line\n",
    "        if len(sequences) != 0:\n",
    "            for s in sequences:\n",
    "                seq_start, seq_end, seq = sequences[0]\n",
    "                if seq_start < index < seq_end:\n",
    "                    seq = seq.replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "                    csv_line[-2] = seq\n",
    "        csv_line[-1] = csv_line[-1].strip(\",\")\n",
    "        csv_file.append(csv_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the output\n",
    "Finally, we need to save the output in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n",
      "File has been saved in 'Aya_output.csv' ok\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# write the csv\n",
    "with open(output_path, \"w\", newline=\"\", encoding=\"utf16\") as f:\n",
    "    writer = csv.writer(f, delimiter=separator)\n",
    "    writer.writerows(csv_file)\n",
    "print(f\"Done!\\nFile has been saved in '{output_path}'\",\"ok\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there were some unk categories, print them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have found several categories not listed in 'TranscriptionTagger/independent_variables.json' or in 'TranscriptionTagger/dependent_variables.json'.\n",
      "Following in alphabetical order:\n",
      "None\n",
      "\n",
      "None\n",
      "OTH-DIAL\n",
      "None\n",
      "DEM-HA-GEB\n",
      "None\n",
      "DEM-HAADHI\n",
      "None\n",
      "DL\n",
      "None\n",
      "ENGL-SCHOOL\n",
      "None\n",
      "EPENTH-VOW\n",
      "None\n",
      "G\n",
      "None\n",
      "G-SCHOOl\n",
      "None\n",
      "GA\n",
      "None\n",
      "GE\n",
      "None\n",
      "IA-AYA\n",
      "None\n",
      "JOB\n",
      "None\n",
      "K\n",
      "None\n",
      "L\n",
      "None\n",
      "MAAl\n",
      "None\n",
      "NO-A-END\n",
      "None\n",
      "NO-HA\n",
      "None\n",
      "NO-HA-SUF\n",
      "None\n",
      "NO-HA-SUFF\n",
      "None\n",
      "NO-IINA\n",
      "None\n",
      "NO-MAAl\n",
      "None\n",
      "NO-UUN-SUF\n",
      "None\n",
      "No-IINA\n",
      "None\n",
      "No-MAAL\n",
      "None\n",
      "OT-SA\n",
      "None\n",
      "OTH\n",
      "None\n",
      "OTH-SA-FEAT\n",
      "None\n",
      "OTH_SA\n",
      "None\n",
      "SCHOOL\n",
      "None\n",
      "SS-DIFF\n",
      "None\n",
      "SS-EPENTH-VOW\n",
      "None\n",
      "SS-HA\n",
      "None\n",
      "SS-RAISE\n",
      "None\n",
      "SUF-WITH-IINA\n",
      "None\n",
      "SUF-YA\n",
      "None\n",
      "higiina ʃ\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if len(unk_categories) > 0:\n",
    "    unk_categories = set(unk_categories)\n",
    "    unk_categories = sorted(unk_categories)\n",
    "    print(print(\n",
    "        f\"I have found several categories not listed in '{independent_variable_path}' or in '{dependent_variable_path}'.\\n\"\n",
    "        f\"Following in alphabetical order:\"))\n",
    "    for c in unk_categories:\n",
    "        print(print(c.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
