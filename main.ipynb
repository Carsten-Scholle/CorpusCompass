{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 5\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcopy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m copy\n\u001B[0;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolab\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m files\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "from copy import copy\n",
    "from google.colab import files\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom files and settings\n",
    "\n",
    "In this section we will define some settings for your transcription!\n",
    "We will start with the variables you defined and move on to how to find them in your annotated corpus (REGEX).\n",
    "\n",
    "## Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining variables with JSON\n",
    "When annotating your data, you will for sure use some variables. These variables may have a hierarchical structure, where one category includes many variations. To allow the program to find your variables you have to build a \"dictionary\" where you specify them.\n",
    "Here, we use JSON files that allow you to come up with how many categories and variables you want in a clear and defined manner. Check out the [introduction to JSON tutorial](https://www.w3schools.com/js/js_json_intro.asp) if you are not familiar with it.\n",
    "You can also look at the  [dependent variables](./dependent_variables.json) we are using in this project.\n",
    "\n",
    "### Your files\n",
    "Now that you are familiar with how the JSON syntax work, we need to define what your variables are. Here you have two options, depending on the option you choose you will need to run different cells.\n",
    "\n",
    "#### 1. Use the default variables file\n",
    "At the moment there are two files, one for [dependent variables](./dependent_variables.json) and one for [independent variables](./independent_variables.json) in this code. If you open these link you will see the original files (not modificable), but you can open them from this web page on the top right (check out [this tutorial](https://neptune.ai/blog/google-colab-dealing-with-files) on how to access local files system to google colab, point 4), you can find the files inside the `TranscriptionTagger` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_variable_path = 'TranscriptionTagger/dependent_variables.json'\n",
    "independent_variable_path = 'TranscriptionTagger/independent_variables.json'\n",
    "\n",
    "variable_files=[dependent_variable_path, independent_variable_path]\n",
    "variable_files=[open(path, \"r+\") for path in variable_files]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Upload your own files\n",
    "Here you can upload your own variable files as long as they are still in JSON format. You can upload how many files you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m variable_files\u001B[38;5;241m=\u001B[39m\u001B[43mfiles\u001B[49m\u001B[38;5;241m.\u001B[39mupload()\n",
      "\u001B[0;31mNameError\u001B[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "variable_files=files.uploads()\n",
    "\n",
    "for fn in variable_files.keys():\n",
    "    if \".json\" not in fn:\n",
    "        raise FileNotFoundError(f\"File {fn} is not a JSON file!\")\n",
    "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(variable_files[fn])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions (REGEX)\n",
    "\n",
    "Now it's time to define the regular regular expressions (REGEX) to find your annotation in the corpus.\n",
    "\n",
    "In our project each annotation is contained in square brackets and starts with a dollar sign. The variables are divided by a dot (without spaces) and the last element after the dot is the annotated word (with spaces).\n",
    "For example : `[$variable1.variable2.annotated word]`\n",
    "\n",
    "To come up with a new tag REGEX you can use [regex101](https://regex101.com/). To check out how it works, open  [regex101](https://regex101.com/). Copy-paste the\n",
    "content of `square_regex` (`(\\[\\$[\\S ]*?\\])`) into the regular expression bar (on the top) and a sample paragraph in the test string (on the bottom), e.g.:\n",
    "```\n",
    "S    akiid, akiid, bi l [$G-OTH.fooxinende], aa, yaʕni il jumʕa la bass ʕidna sabit uu aħħad iħna [$DEM-HAAY.haay] [$G-OTH.daayrakt] leen il alwaad ʕidna       aa, iða j jaww [$IA.kulliʃ] [$IA.zeen] insawwi maʃaawi, w iða j jaww mu [$IA.zeen], insawwi yaʕni l aklaat illi tijmaʕ il ʕaaʔila, tiðakkiriin iħna l ʕiraaqiyiin         id dooLMa, w is [$CK.simaC], (laughing), w il lamma l ħilwa w il aħfaad, fa insawwi [$IA.CK.hiiCi]          bass il yoom la yaʕni innu aani w il ħajji [$GQ.gaaʕdiin], akθar il marraat nugʕud iS SuBiħ [$IA.nitrayyag], baʕdeen il, il gahwa uu baʕdeen nuqʕud insoolif, inʃuuf [$IA.ʃinu] ʕidna maʃaariiʕ, niTLaʕ maθalan irruuħ nimʃi [$IA.fadd] niSS saaʕa saaʕa\n",
    "```\n",
    "\n",
    "Define the REGEXs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square_regex = re.compile(r\"(\\[\\$[\\S ]*?\\])\")\n",
    "feat_regex = re.compile(r'\\[\\$([\\S ]*?)\\]')\n",
    "sequence_regex = re.compile(r\"({[\\S ]+})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transcription file\n",
    "Finally, you need to upload your transcription file. This is the filed containing your annotated corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcription_path = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output file\n",
    "\n",
    "In order to obtain the comma separated values (`csv`) file, you can specify the separator. By default the separator value is a comma (as the name implies), but you can also use:\n",
    "- semicolon: `;`\n",
    "- comma: `,`\n",
    "- tab : `\\t`\n",
    "\n",
    "In our project we use the tab, since it is the one that allows excel to view it (for Germany the default value is tab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = '\\t'"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Helper functions\n",
    "Following some functions that we will use later in the code."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_features(corpus):\n",
    "    \"\"\"\n",
    "    Remove the features from the corpus\n",
    "    \"\"\"\n",
    "    corpus = copy(corpus)\n",
    "    words = square_regex.findall(corpus)\n",
    "    for w in words:\n",
    "        try:\n",
    "            text = w.rsplit(\".\", 1)[1][:-1]\n",
    "            corpus = corpus.replace(w, text)\n",
    "        except IndexError:\n",
    "            print(f\"I found an error for the tag '{w}'. Myabe it does not have a point in it?\\n\"\n",
    "                        f\"Please check the tag and try again.\", \"error\")\n",
    "            exit()\n",
    "            continue\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_name(line):\n",
    "    return line.split(\" \")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Main program\n",
    "Time to start the main program. First let us merge all the variables we uploaded before"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_files=[json.load(f) for f in variable_files]\n",
    "\n",
    "variable_dict = {}\n",
    "\n",
    "for json in variable_files:\n",
    "    for k,v in json.items():\n",
    "        if k in variable_dict.keys():\n",
    "            print(f\"Warning : I have found a duplicate variable named '{k}' with values '{v}'. \"\n",
    "                  f\"If this is expected ignore this message, otherwise check the variables files for duplicates!\")\n",
    "        variable_dict[k]=v\n",
    "\n",
    "\n",
    "\n",
    "# get an inverse of the dependent variable\n",
    "idv = {}\n",
    "for k, v in variable_dict.items():\n",
    "    if isinstance(v, list):\n",
    "        for i in v:\n",
    "            idv[i] = k\n",
    "    else:\n",
    "        idv[v] = k\n",
    "\n",
    "print(\"Correctly loaded all the variables. Check them out to see if there are any errors\")\n",
    "print(json.dumps(variable_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# get the name of the output path\n",
    "output_path=os.path.basename(transcription_path)\n",
    "output_path=os.path.splitext(output_path)[0]+\"_output.csv\"\n",
    "\n",
    "# opend the file\n",
    "with open(transcription_path, 'r+', encoding=\"utf16\") as f:\n",
    "    trans = f.readlines()\n",
    "trans = trans[2:]\n",
    "trans = [x.strip() for x in trans]\n",
    "trans = [x for x in trans if x != '']\n",
    "\n",
    "# ask for interviwers name\n",
    "interviewers = input(print(f'Add the name(s) of the interviewer(s) (separated by comma), '\n",
    "                          f'leave empty for classical interviewer-interviewees structure: '))\n",
    "print(\"\\n\")\n",
    "if len(interviewers) > 0:\n",
    "    interviewers = interviewers.split(',')\n",
    "else:\n",
    "    interviewers = trans[0][0]\n",
    "\n",
    "# ask for previous line\n",
    "previous_line = input(print(f'When generating the final cvs file, I can also include the speaker utterance.'\n",
    "                           f' Do you want me to include it? (y/n): '))\n",
    "print(\"\\n\")\n",
    "if previous_line == 'y':\n",
    "    previous_line = True\n",
    "else:\n",
    "    previous_line = False\n",
    "\n",
    "# get speak/list names\n",
    "names = [get_name(x).strip() for x in trans]\n",
    "names = set(names)\n",
    "\n",
    "# remove all mention of interviwers in names\n",
    "for i in interviewers:\n",
    "    names = [x for x in names if i not in x]\n",
    "interviewees = list(names)\n",
    "\n",
    "# notify user about names\n",
    "print(print(f\"I found the following names: {', '.join(interviewees)}\"))\n",
    "\n",
    "# compile regex to find features\n",
    "csv_header = list(dependent_variable.keys())\n",
    "\n",
    "# define the end of the csv\n",
    "csv_end = ['sequence in sentence', 'unk']\n",
    "if previous_line:\n",
    "    csv_end.insert(0, 'previous line')\n",
    "csv_header = [\"text\"] + csv_header + csv_end\n",
    "csv_file = [csv_header]\n",
    "unk_categories = []\n",
    "\n",
    "# for every paragraph in the transcript\n",
    "for idx in range(len(trans)):\n",
    "    c = trans[idx]\n",
    "\n",
    "    # get the paragraph without features\n",
    "    if get_name(c) in interviewees:\n",
    "        sp = trans[idx - 1]\n",
    "    else:\n",
    "        continue\n",
    "    clean_p = remove_features(c)\n",
    "\n",
    "    # capture all the sequences\n",
    "    sequences = sequence_regex.finditer(clean_p)\n",
    "    sequences = [(x.start(), x.end(), x.group()) for x in sequences]\n",
    "\n",
    "    # get the features\n",
    "    tags = feat_regex.finditer(c)\n",
    "\n",
    "    # for every tags with features in the paragraph\n",
    "    for t in tags:\n",
    "        # get index of result + tag\n",
    "        index = t.start()\n",
    "        t = t.group(1)\n",
    "\n",
    "        # initialize empty row\n",
    "        csv_line = [\"\" for _ in range(len(csv_header))]\n",
    "\n",
    "        # get the features\n",
    "        feats = t.rsplit(\".\", 1)\n",
    "        text = feats[1]\n",
    "        feats = feats[0]\n",
    "\n",
    "        # for every feature in the word\n",
    "        for f in feats.split(\".\"):\n",
    "            # if the category is not present in the dict, then add to unk\n",
    "            if f not in idv.keys():\n",
    "                unk_categories.append(f)\n",
    "                csv_line[-1] = csv_line[-1] + f + \",\"\n",
    "            else:\n",
    "                category = idv[f]\n",
    "                cat_idx = csv_header.index(category)\n",
    "                csv_line[cat_idx] = f\n",
    "\n",
    "        # add initial infos and final unk to the line\n",
    "        csv_line[0] = text\n",
    "        if previous_line:\n",
    "            csv_line[-3] = sp\n",
    "\n",
    "        # add the sequence to the line\n",
    "        if len(sequences) != 0:\n",
    "            for s in sequences:\n",
    "                seq_start, seq_end, seq = sequences[0]\n",
    "                if seq_start < index < seq_end:\n",
    "                    seq = seq.replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "                    csv_line[-2] = seq\n",
    "        csv_line[-1] = csv_line[-1].strip(\",\")\n",
    "        csv_file.append(csv_line)\n",
    "\n",
    "# write the csv\n",
    "with open(output_path, \"w\", newline=\"\", encoding=\"utf16\") as f:\n",
    "    writer = csv.writer(f, delimiter=separator)\n",
    "    writer.writerows(csv_file)\n",
    "print(f\"Done!\\nFile has been saved in '{output_path}'\",\"ok\")\n",
    "if len(unk_categories) > 0:\n",
    "    unk_categories = set(unk_categories)\n",
    "    unk_categories = sorted(unk_categories)\n",
    "    print(print(\n",
    "        f\"I have found several categories not listed in '{independent_variable_path}' or in '{dependent_variable_path}'.\\n\"\n",
    "        f\"Following in alphabetical order:\"))\n",
    "    for c in unk_categories:\n",
    "        print(print(c.strip()))"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
