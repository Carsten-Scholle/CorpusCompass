{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "This file ....\n",
    "\n",
    "\n",
    "## Setup\n",
    "Run this cell for the setups"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/nicofirst1/TranscriptionTagger\n",
    "!pip install nltk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "from utils import get_name, count_tokens\n",
    "from Configs import Configs\n",
    "\n",
    "configs=Configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corpus file\n",
    "Finally, you need to upload your corpus file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONT USE THIS, IT IS JUST FOR DEBUG\n",
    "if False:\n",
    "    corpus_path = \"/Users/giulia/Downloads/Telegram Desktop/input.txt\"\n",
    "    with open(corpus_path, \"r+\", encoding=\"utf16\") as f:\n",
    "        corpus_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "corpus_path = files.upload()\n",
    "\n",
    "assert len(corpus_path) ==1 , \"Support for multiple transcription file is not available! Please upload just one file\"\n",
    "corpus_text=list(corpus_path.values())[0]\n",
    "corpus_path=list(corpus_path.keys())[0]\n",
    "\n",
    "corpus_text=corpus_text.decode(\"utf16\")\n",
    "\n",
    "# print first 50 character of the transcription file\n",
    "print(\"\\n\\n First 400 characters:\\n\")\n",
    "corpus_text[:400]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output file\n",
    "<br>\n",
    "In order to obtain the comma separated values (`csv`) file, you can specify the separator. By default the separator value is a comma (as the name implies), but you can also use:<br>\n",
    "- semicolon: `;`<br>\n",
    "- comma: `,`<br>\n",
    "- tab : `\\t`<br>\n",
    "<br>\n",
    "In our project we use the tab, since it is the one that allows excel to view it (for Germany the default value is tab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = configs.separator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program<br>\n",
    "Time to start the main program. First let us merge all the variables we uploaded before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: split the whole corpus in different elements every new line, creating a list of paragraphs. For us this means splitting interviewer and interviewee in different paragraphs\n",
    "trans = corpus_text.split(\"\\n\")\n",
    "# step 2: our first two paragraphs are the file name and the date, which we don't need so discard them\n",
    "trans = trans[2:]\n",
    "# step 3: remove spaces at the start and end of each paragraph\n",
    "trans = [x.strip() for x in trans]\n",
    "# step 4 : remove empty paragraphs from the list\n",
    "trans = [x for x in trans if x != '']\n",
    "\n",
    "# take a peek at the first four paragraphs\n",
    "for idx in range(3):\n",
    "    print(trans[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interviewers and Interviewees\n",
    "As you can see form the previous example, we consider a file where we have only one interviewer and one interviewee alternating each other. But it can be the case where you have multiple interviewers and interviewees in random order. In this case we need to know the names of the interviewers in order to split them from the interviewees.\n",
    "If you need to add multiple interviewers run the following cell after adding the names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interviewers = \"name1,name2,...,nameN\"\n",
    "interviewers = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the interviewers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if user specified the interviewer's names then take that\n",
    "if len(interviewers) > 0:\n",
    "    interviewers = interviewers.split(',')\n",
    "    # remove spaces\n",
    "    interviewers = [x.strip() for x in interviewers]\n",
    "else:\n",
    "    # else use the first character of the transcription\n",
    "    interviewers = [trans[0][0]]\n",
    "print(f\"The selected interviewers are: {', '.join(interviewers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output file settings<br>\n",
    "Now it's time to create the output file.\n",
    "\n",
    "The output file will have the same name as the transcription one but with `_analysis.csv` at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the name of the output path\n",
    "output_path = os.path.basename(corpus_path)\n",
    "output_path = os.path.splitext(output_path)[0] + \"_analysis.csv\"\n",
    "output_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding interviewees\n",
    "\n",
    "This part looks for all the names present in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get interviewer/interviewees names\n",
    "\n",
    "names = [get_name(x).strip() for x in trans]\n",
    "names = set(names)\n",
    "\n",
    "# remove all mention of interviwers in names\n",
    "\n",
    "for i in interviewers:\n",
    "    names = [x for x in names if i not in x]\n",
    "interviewees = list(names)\n",
    "\n",
    "# notify user about names\n",
    "\n",
    "print(f\"I found the following interviewees names: {', '.join(interviewees)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the csv head\n",
    "\n",
    "Our csv contains the following informations:\n",
    "- the considered token (or word)\n",
    "- how many times was repeated in all the corpus\n",
    "- for each speaker, how many times it was repeated for that speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers = interviewees + interviewers\n",
    "csv_header = ['token', 'total repetitions'] + [f\"repetitions in {x}\" for x in speakers]\n",
    "print(f\"The csv header looks like this\")\n",
    "csv_header"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create the data for analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "speaker_specific_data = {k: [x.replace(k, \"\") for x in trans if get_name(x) == k] for k in speakers}\n",
    "# remove names from data\n",
    "speaker_specific_data = {k: \"\\n\".join(v) for k, v in speaker_specific_data.items()}\n",
    "# create corpus as string\n",
    "corpus = \"\\n\".join(trans)\n",
    "# remove speaker names from corpus\n",
    "\n",
    "for k in speakers:\n",
    "    corpus = corpus.replace(k, \"\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Depending on the lenght of your corpus, running the following lines may take a bit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_words = count_tokens(corpus)\n",
    "speaker_specific_words = {k: count_tokens(v) for k, v in speaker_specific_data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Once the results are ready, build the csv file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_rows = [csv_header]\n",
    "for token in corpus_words.keys():\n",
    "    csv_row = [token, corpus_words[token]]\n",
    "    for sp in speakers:\n",
    "        if token in speaker_specific_words[sp]:\n",
    "            csv_row.append(speaker_specific_words[sp][token])\n",
    "        else:\n",
    "            csv_row.append(0)\n",
    "    csv_rows.append(csv_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the results and download"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_path, 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile, delimiter=',')\n",
    "    csvwriter.writerows(csv_rows)\n",
    "\n",
    "files.download(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
