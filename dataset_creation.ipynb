{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "This code is meant for automatically transform an annotated corpora to a dataset in `csv` format.\n",
    "While we do have a standard for annotations, instruction to adapt the code to your annotation syntax are available below.\n",
    "\n",
    "If you find any problems or request some features update please do so with [this form](https://github.com/nicofirst1/TranscriptionTagger/issues/new/choose)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup\n",
    "\n",
    "Run this to copy the necessary files in your colab.\n",
    "If it says something like:\n",
    "```fatal: destination path 'TranscriptionTagger' already exists and is not an empty directory.```\n",
    "You can ignore it"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/nicofirst1/TranscriptionTagger\n",
    "!pip install nltk\n",
    "!pip install -e TranscriptionTagger/"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from TranscriptionTagger.utils import remove_features, get_name, get_ngram, find_repetitions, multi_corpus_upload\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom files and settings\n",
    "\n",
    "In this section we will define some settings for your transcription!\n",
    "We will start with the variables you defined and move on to how to find them in your annotated corpus (REGEX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "### Defining variables with JSON\n",
    "When annotating your data, you will for sure use some variables. These variables may have a hierarchical structure, where one category includes many variations. To allow the program to find your variables you have to build a \"dictionary\" where you specify them.\n",
    "Here, we use JSON files that allow you to come up with how many categories and variables you want in a clear and defined manner. Check out the [introduction to JSON tutorial](https://www.w3schools.com/js/js_json_intro.asp) if you are not familiar with it.\n",
    "You can also look at the  [dependent variables](./dependent_variables.json) we are using in this project.\n",
    "\n",
    "### Your files\n",
    "Now that you are familiar with how the JSON syntax work, we need to define what your variables are. Here you have two options, depending on the option you choose you will need to run different cells.\n",
    "\n",
    "#### 1. Use the default variables file\n",
    "At the moment there are two files, one for [dependent variables](./dependent_variables.json) and one for [independent variables](./independent_variables.json) in this code. If you open these link you will see the original files (not modificable). If you want to modify them you need to open them from this page. On the folder symbol on the left of this code (check [this video](./includes/changing_variables.gif) for a how-to and  [this tutorial](https://neptune.ai/blog/google-colab-dealing-with-files) on how to access local files system to google colab, point 4), click on TranscriptionTagger (this will open the direcotry) and then you will see the two files. Click on them to modify their content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependent_variable_path = 'TranscriptionTagger/dependent_variables.json'\n",
    "independent_variable_path = 'TranscriptionTagger/independent_variables.json'\n",
    "\n",
    "variable_files=[dependent_variable_path, independent_variable_path]\n",
    "variable_files=[open(path, \"r+\") for path in variable_files]\n",
    "variable_files=[x.read() for x in variable_files]\n",
    "variable_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Upload your own files\n",
    "Here you can upload your own variable files as long as they are still in JSON format. You can upload how many files you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_files=files.upload()\n",
    "\n",
    "for fn in variable_files.keys():\n",
    "    if \".json\" not in fn:\n",
    "        raise FileNotFoundError(f\"File {fn} is not a JSON file!\")\n",
    "    print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(variable_files[fn])))\n",
    "\n",
    "variable_files=list(variable_files.values())\n",
    "\n",
    "# decode\n",
    "variable_files=[x.decode(\"utf8\") for x in variable_files]\n",
    "variable_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions (REGEX)\n",
    "\n",
    "Now it's time to define the regular regular expressions (REGEX) to find your annotation in the corpus.\n",
    "\n",
    "In our project each annotation is contained in square brackets and starts with a dollar sign. The variables are divided by a dot (without spaces) and the last element after the dot is the annotated word (with spaces).\n",
    "For example : `[$variable1.variable2.annotated word]`\n",
    "\n",
    "To come up with a new tag REGEX you can use [regex101](https://regex101.com/). To check out how it works, open  [regex101](https://regex101.com/). Copy-paste the\n",
    "content of `square_regex` (`(\\[\\$[\\S ]*?\\])`) into the regular expression bar (on the top) and a sample paragraph in the test string (on the bottom), e.g.:\n",
    "```\n",
    "S    akiid, akiid, bi l [$G-OTH.fooxinende], aa, yaʕni il jumʕa la bass ʕidna sabit uu aħħad iħna [$DEM-HAAY.haay] [$G-OTH.daayrakt] leen il alwaad ʕidna       aa, iða j jaww [$IA.kulliʃ] [$IA.zeen] insawwi maʃaawi, w iða j jaww mu [$IA.zeen], insawwi yaʕni l aklaat illi tijmaʕ il ʕaaʔila, tiðakkiriin iħna l ʕiraaqiyiin         id dooLMa, w is [$CK.simaC], (laughing), w il lamma l ħilwa w il aħfaad, fa insawwi [$IA.CK.hiiCi]          bass il yoom la yaʕni innu aani w il ħajji [$GQ.gaaʕdiin], akθar il marraat nugʕud iS SuBiħ [$IA.nitrayyag], baʕdeen il, il gahwa uu baʕdeen nuqʕud insoolif, inʃuuf [$IA.ʃinu] ʕidna maʃaariiʕ, niTLaʕ maθalan irruuħ nimʃi [$IA.fadd] niSS saaʕa saaʕa\n",
    "```\n",
    "\n",
    "Define the REGEXs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex to find the complete annotation rule\n",
    "square_regex = re.compile(r\"(\\[\\$[\\S ]*?\\])\")\n",
    "# regex to find the content of an annotation\n",
    "feat_regex = re.compile(r'\\[\\$([\\S ]*?)\\]')\n",
    "# regex to find the token in the annotation\n",
    "annotated_token_regex=re.compile(r\"\\.[\\S ]*?\\]\")\n",
    "# regex to univocally finding the speaker name in the paragraph\n",
    "name_regex= re.compile(r\"(^[A-Z]) \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus file\n",
    "Finally, you need to upload your corpus file. This is the filed containing your annotated tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# DONT USE THIS, IT IS JUST FOR DEBUG\n",
    "\n",
    "if False:\n",
    "    corpus_path=\"/Users/giulia/Downloads/Telegram Desktop/input.txt\"\n",
    "    with open(corpus_path, \"r+\", encoding=\"utf16\") as f:\n",
    "        corpus_text=f.read()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_path = files.upload()\n",
    "corpus_text=multi_corpus_upload(corpus_path)\n",
    "\n",
    "print(\"\\n\\n First 400 characters:\\n\")\n",
    "corpus_text[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output file\n",
    "\n",
    "In order to obtain the comma separated values (`csv`) file, you can specify the separator. By default the separator value is a comma (as the name implies), but you can also use:\n",
    "- semicolon: `;`\n",
    "- comma: `,`\n",
    "- tab : `\\t`\n",
    "\n",
    "Be aware that having the same character in your corpus may break the `csv` visualization. We suggest using a symbol that does not appear in your corpus, and then set the separator in the program you are using to visualize (e.g. excel) manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = ';'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program\n",
    "Time to start the main program. First let us merge all the variables we uploaded before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_files=[json.loads(f) for f in variable_files]\n",
    "\n",
    "variable_dict = {}\n",
    "\n",
    "for j in variable_files:\n",
    "    for k,v in j.items():\n",
    "        if k in variable_dict.keys():\n",
    "            print(f\"Warning : I have found a duplicate variable named '{k}' with values '{v}'. \"\n",
    "                  f\"If this is expected ignore this message, otherwise check the variables files for duplicates!\")\n",
    "        variable_dict[k]=v\n",
    "\n",
    "\n",
    "\n",
    "# get an inverse of the dependent variable\n",
    "idv = {}\n",
    "for k, v in variable_dict.items():\n",
    "    if isinstance(v, list):\n",
    "        for i in v:\n",
    "            idv[i] = k\n",
    "    else:\n",
    "        idv[v] = k\n",
    "\n",
    "print(\"Correctly loaded all the variables. Check them out to see if there are any errors\")\n",
    "print(json.dumps(variable_dict,sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing transcriptions\n",
    "Based on your type of annotated corpus you will need to preprocess the file.\n",
    "Feel free to comment out (add `#` at the start of the line) any preprocessing that does not fit your criteria.\n",
    "To give you a sense of our transcriptions here is how the first 5 paragraphs look like:\n",
    "\n",
    "```\n",
    "B       marħaba ʕeeni ʃ axbaariC, iħna niʃakkariC ihwaaya lennahu, ey    fitaħtuulna beetkum uu istaqbaltuuna             hm, iħCiilna l yoom iʃ sawweeti, ʃinu Caan maʃruuʕiC aSLan, (laughing)     hm  ey  hm  ii bi l ʕaafya      ey  ahh  (laughing)  Caan huwwa imxaTTiT innu tijiin inti haaða l isbuuʕ\n",
    "S           halow ħabiibti, halow ʕeeni, il ħam..., (...) baSiiTa itdallili uu haaða abSaT ʃii insawwiilkumiyaa            yaa miyyat hala biikum    il yoom? il yoom ma ʕindi ʃii, aa, mit... mittafqiin ʕal [$MAAL.mawʕid maalatkum] il yoom, fa [$SS-DIF.gaʕadt] iS Subiħ, aa sawweet [$OTH-SA.SS-DIF.ifTuur] aani w il ħajji, rayyagta ab... aLLa [$CK.yʕaafiiC] uu baʕdeen ijeet ʕala mawʕidna [??ihnaana], [$GQ.gaaʕda] antiDurkum, SaLLeet, ma ʕindi ʃii baʕad, da antiDurkum, (laughing)      ey liʔannahu mittafqiin ʕa l mawʕid fa ma [$GQ.nigdar] inɣayyra\n",
    "\n",
    "\n",
    "B   eh, la, yaʕni aa, waħħad ʕan il mawʕid Caan inti yaʕni ʕindiC ɣeer barnaamij maθalan bi l fooxinende        ey  hm  ee, id doo...,(laughing), ii, (laughing), w il lamma l ħilwa, (laughing)   ee  ey          hm  ey  hm, ħeel zeen\n",
    "S    akiid, akiid, bi l [$G-OTH.fooxinende], aa, yaʕni il jumʕa la bass ʕidna sabit uu aħħad iħna [$DEM-HAAY.haay] [$G-OTH.daayrakt] leen il alwaad ʕidna       aa, iða j jaww [$IA.kulliʃ] [$IA.zeen] insawwi maʃaawi, w iða j jaww mu [$IA.zeen], insawwi yaʕni l aklaat illi tijmaʕ il ʕaaʔila, tiðakkiriin iħna l ʕiraaqiyiin         id dooLMa, w is [$CK.simaC], (laughing), w il lamma l ħilwa w il aħfaad, fa insawwi [$IA.CK.hiiCi]          bass il yoom la yaʕni innu aani w il ħajji [$GQ.gaaʕdiin], akθar il marraat nugʕud iS SuBiħ [$IA.nitrayyag], baʕdeen il, il gahwa uu baʕdeen nuqʕud insoolif, inʃuuf [$IA.ʃinu] ʕidna maʃaariiʕ, niTLaʕ maθalan irruuħ nimʃi [$IA.fadd] niSS saaʕa saaʕa\n",
    "\n",
    "```\n",
    "As you can see our file has a repeating structure of the kind:\n",
    "- interviewer name (`B`), space, paragraph, newline\n",
    "- interviewee name (`S`), space, paragraph, newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: split the whole corpus in different elements every new line, creating a list of paragraphs. For us this means splitting interviewer and interviewee in different paragraphs\n",
    "corpus= corpus_text.split(\"\\n\")\n",
    "# step 2: remove spaces at the start and end of each paragraph\n",
    "corpus = [x.strip() for x in corpus]\n",
    "# step 3 : remove empty paragraphs from the list\n",
    "corpus = [x for x in corpus if x != '']\n",
    "# step 4 : filter out all the paragraphs that do not have any DETECTED speaker\n",
    "prev_c=copy(corpus)\n",
    "corpus=[x for x in corpus if get_name(x,name_regex)]\n",
    "\n",
    "removed_sentences=len(prev_c)-len(corpus)\n",
    "print(f\"I removed {removed_sentences} paragraphs, since I could not detect a speaker\")\n",
    "if removed_sentences>0:\n",
    "    diff=set(prev_c)-set(corpus)\n",
    "    print(\"the removed lines are: \")\n",
    "    diff\n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "# take a peek at the first four paragraphs\n",
    "print(\"First paragraphs are\")\n",
    "for idx in range(3):\n",
    "    print(corpus[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interviewers and Interviewees\n",
    "As you can see form the previous example, we consider a file where we have only one interviewer and one interviewee alternating each other. But it can be the case where you have multiple interviewers and interviewees in random order. In this case we need to know the names of the interviewers in order to split them from the interviewees.\n",
    "\n",
    "If you need to add multiple interviewers run the following cell after adding the names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interviewers = \"name1,name2,...,nameN\"\n",
    "interviewers=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the interviewers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if user specified the interviewer's names then take that\n",
    "if len(interviewers) > 0:\n",
    "    interviewers = interviewers.split(',')\n",
    "    # remove spaces\n",
    "    interviewers = [x.strip() for x in interviewers]\n",
    "else:\n",
    "    # else use the first character of the transcription\n",
    "    interviewers = [corpus[0][0]]\n",
    "     # filter out empty names\n",
    "    interviewers = [x for x in interviewers if x != '']\n",
    "    interviewers=[interviewers[0]]\n",
    "\n",
    "print(f\"The selected interviewers are: {', '.join(interviewers)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output file settings\n",
    "Now it's time to create the output files.\n",
    "\n",
    "There are different outputs and here we specify their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the name of the output path\n",
    "dataset_path=\"dataset.csv\"\n",
    "annoation_info_path=\"annotation_info.csv\"\n",
    "not_annotated_path=\"not_annotated_log.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to add the previous paragraph in the final csv file. For example, when examining an annotation, you want to know what the previous speaker said before the current one. If you are interested in this information being in the final output set `previous_line` to `True`, else leave it `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#previous_line = True\n",
    "previous_line = False"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Additionally, we include a context for all the annotated token present in your data. The context is made of the previous/next words after the annotated one, following a n-gram rule. Following, you can se the size of your context:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ngram_params = (previous words, next words)\n",
    "ngram_params=(10,5)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a csv file and it needs a header. For this reason here we define the header as the following elements:\n",
    "- the `token` for the annotated words\n",
    "- the complete list of variables\n",
    "- (Optional) the previous line\n",
    "- The context in which the text was found\n",
    "- an `unk` (unknown) category for variables that were found in the annotations but not present in the variable files (useful to catch some errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile regex to find features\n",
    "csv_header = list(variable_dict.keys())\n",
    "\n",
    "# define the end of the csv\n",
    "csv_end = ['context', 'unk']\n",
    "if previous_line:\n",
    "    csv_end.insert(0, 'previous line')\n",
    "csv_header = [\"token\"] + csv_header + csv_end\n",
    "csv_file = [csv_header]\n",
    "unk_categories = []\n",
    "\n",
    "print(f\"The csv header looks like this\")\n",
    "csv_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding interviewees\n",
    "\n",
    "This part looks for all the names present in the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get interviewer/interviewees names\n",
    "names = [get_name(x, name_regex).strip() for x in corpus]\n",
    "names = set(names)\n",
    "# filter out empty names\n",
    "names = [x for x in names if x != '']\n",
    "\n",
    "# remove all mention of interviwers in names\n",
    "for i in interviewers:\n",
    "    names = [x for x in names if i not in x]\n",
    "interviewees = list(names)\n",
    "\n",
    "# notify user about names\n",
    "print(f\"I found the following interviewees names: {', '.join(interviewees)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Annotation information\n",
    "\n",
    "Here we build our counter and logger for the annotations. We want to count the frequency of each annotated token in the corpus, how many times it appears and who speaks it. Moreover, for logging reason, we want to present the tokens which are annotated but were found outside the annotation rule."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# ### Finding all the annotated words\n",
    "whole_corpus=\"\\n\".join(corpus)\n",
    "annotations=feat_regex.findall(whole_corpus)\n",
    "annotations=[x.split(\".\")[-1] for x in annotations]\n",
    "\n",
    "# count the number of annotations\n",
    "annotation_counter = Counter(annotations)\n",
    "annotation_counter={k:dict(annotated=v) for k,v in annotation_counter.items()}\n",
    "\n",
    "print(f\"The total number of annotated words is {len(annotation_counter)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we check, for each annotated token, the number of times it appear inside the annotation format vs outside."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# check if there are any annotations not annotated\n",
    "for k,v in annotation_counter.items():\n",
    "    # check for annotation repetitions\n",
    "    wild_rep,ann_rep,_= find_repetitions(whole_corpus, k, annotated_token_regex)\n",
    "    total_rep=wild_rep+ann_rep\n",
    "    not_annotated=total_rep-v['annotated']\n",
    "    annotation_counter[k]['not_annotated']=not_annotated\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While the previous cell, counts all the tokens in the whole corpus, we probably want to differentiate between speakers. For example, it may be the case where we do not annotate tokens from interviewers, focusing only on interviewees.\n",
    "\n",
    "For this reason, here we need to specify where we want to look for the missed tokens."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# choose which speaker to check for annotations, you can uncomment one of the following lines:\n",
    "# speakers=interviewers+interviewees\n",
    "# speakers=interviewers\n",
    "# speakers=[\"name1\",\"name2\",...,\"nameN\"]\n",
    "\n",
    "speakers=interviewees"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now it's time to find those missing annotations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "not_annotated_log={}\n",
    "\n",
    "for k,v in annotation_counter.items():\n",
    "\n",
    "    res=[find_repetitions(x, k, annotated_token_regex) for x in corpus if get_name(x,name_regex) in speakers]\n",
    "    _,_,wild_not_annotated=zip(*res)\n",
    "\n",
    "    # unfold the list\n",
    "    wild_not_annotated=[item for sublist in wild_not_annotated for item in sublist]\n",
    "\n",
    "    # if there are not annotated words\n",
    "    if len(wild_not_annotated) > 0:\n",
    "        not_annotated_log[k]=wild_not_annotated"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, some post-processing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# augment annotation_counter with speakers and add total number\n",
    "for k in annotation_counter.keys():\n",
    "    annotation_counter[k]['total'] = annotation_counter[k]['annotated'] + annotation_counter[k]['not_annotated']\n",
    "    for speaker in interviewees+interviewers:\n",
    "        annotation_counter[k][speaker] = 0\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the main loop\n",
    "This part starts the main loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for every paragraph in the transcript\n",
    "for idx in range(len(corpus)):\n",
    "    c = corpus[idx]\n",
    "    cur_speaker = get_name(c,name_regex)\n",
    "\n",
    "    # add speaker related metrics\n",
    "    for k, v in annotation_counter.items():\n",
    "        wild_rep, ann_rep, _ = find_repetitions(c, k, annotated_token_regex)\n",
    "        rep = wild_rep + ann_rep\n",
    "        annotation_counter[k][cur_speaker] += rep\n",
    "\n",
    "    # get the paragraph without features\n",
    "    if cur_speaker in interviewees:\n",
    "        sp = corpus[idx - 1]\n",
    "    else:\n",
    "        continue\n",
    "    clean_p = remove_features(c, square_regex)\n",
    "\n",
    "    # get the features\n",
    "    tags = feat_regex.finditer(c)\n",
    "\n",
    "    # for every tags with features in the paragraph\n",
    "    for t in tags:\n",
    "        # get index of result + tag\n",
    "        index = t.start()\n",
    "        t = t.group(1)\n",
    "\n",
    "        # initialize empty row\n",
    "        csv_line = [\"\" for _ in range(len(csv_header))]\n",
    "\n",
    "        # get the features\n",
    "        feats = t.rsplit(\".\", 1)\n",
    "        text = feats[1]\n",
    "        feats = feats[0]\n",
    "\n",
    "        context=get_ngram(text,clean_p,ngram_params)\n",
    "\n",
    "        # for every feature in the word\n",
    "        for f in feats.split(\".\"):\n",
    "            # if the category is not present in the dict, then add to unk\n",
    "            if f not in idv.keys():\n",
    "                unk_categories.append(f)\n",
    "                csv_line[-1] = csv_line[-1] + f + \",\"\n",
    "            else:\n",
    "                category = idv[f]\n",
    "                cat_idx = csv_header.index(category)\n",
    "                csv_line[cat_idx] = f\n",
    "\n",
    "        # add initial infos and final unk to the line\n",
    "        csv_line[0] = text\n",
    "        csv_line[-2] = context\n",
    "        if previous_line:\n",
    "            csv_line[-3] = sp\n",
    "\n",
    "        csv_line[-1] = csv_line[-1].strip(\",\")\n",
    "        csv_file.append(csv_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the output\n",
    "Finally, we need to save the output in the csv file for all our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# write the csv\n",
    "with open(dataset_path, \"w\", newline=\"\", encoding=\"utf16\") as f:\n",
    "    writer = csv.writer(f, delimiter=separator)\n",
    "    writer.writerows(csv_file)\n",
    "\n",
    "\n",
    "# generate the annotation info file\n",
    "header=[\"token\"]+list(list(annotation_counter.values())[0].keys())\n",
    "\n",
    "with open(annoation_info_path, \"w\", newline=\"\", encoding=\"utf16\") as f:\n",
    "    writer = csv.writer(f, delimiter=separator)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for k,v in annotation_counter.items():\n",
    "        writer.writerow([k]+list(v.values()))\n",
    "\n",
    "\n",
    "# save the not annotated log\n",
    "header=[\"token\"]+[f\"context {i}\" for i in range(max(len(x) for x in not_annotated_log.values()))]\n",
    "with open(not_annotated_path, \"w\", newline=\"\", encoding=\"utf16\") as f:\n",
    "    writer = csv.writer(f, delimiter=separator)\n",
    "    writer.writerow(header)\n",
    "    for k, v in not_annotated_log.items():\n",
    "        writer.writerow([k]+v)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you want to download it right away run this cell:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "files.download(dataset_path)\n",
    "files.download(annoation_info_path)\n",
    "files.download(not_annotated_path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unk categories\n",
    "Here, we print the unknown category if we found any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(unk_categories) > 0:\n",
    "    unk_categories = set(unk_categories)\n",
    "    unk_categories = sorted(unk_categories)\n",
    "    print(print(\n",
    "        f\"I have found several categories not listed in your variable file.\\n\"\n",
    "        f\"Following in alphabetical order:\"))\n",
    "    for idx,c in enumerate(unk_categories):\n",
    "        print(idx,f\"'{c}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
