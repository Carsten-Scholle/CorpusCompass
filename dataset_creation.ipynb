{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This tool is meant for researchers in the field of Corpus Linguistics and work on e.g. language variation.\n",
    "\n",
    "Given your corpus and a list of variables, this code will build a structured dataset (in `csv`) for your future analysis. Moreover, it will help you to spot inconsistencies into your annotation and provide a clear overview of how your annotated features correlate with speakers. \n",
    "\n",
    "While we do have a standard for annotations (ELAN and Praat), instructions to adapt the code to your own syntax are available below.\n",
    "\n",
    "If you find any problems or request some features update, please do so with [this form](https://github.com/nicofirst1/CorpusCompass/issues/new/choose).\n",
    "\n",
    "## How does it work?\n",
    "\n",
    "This file you are viewing is called a google Colab. It allows you to run the code (python in this case) in an interactive manner by clicking on each `code cell`. If you are not familiar with google Colab, we advise you to check the [tutorial](https://colab.research.google.com/notebooks/intro.ipynb).\n",
    "\n",
    "When you run each cell, you will see (most of the time) some additional information appearing regarding the status of the program, e.g. number of annotated words, varaibles, etc ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Run this to copy the necessary files in your colab.\n",
    "If it says something like:\n",
    "```fatal: destination path 'CorpusCompass' already exists and is not an empty directory.```\n",
    "You can ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone -b dev https://github.com/nicofirst1/CorpusCompass\n",
    "!pip install nltk\n",
    "!pip install -e CorpusCompass/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "from CorpusCompass.utils import multi_corpus_upload, split_paragraphs, get_name, find_repetitions, remove_features, get_ngram\n",
    "from copy import copy\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom files and settings\n",
    "\n",
    "In this section, we will define some settings for your transcription!\n",
    "\n",
    "We will start with the variables you defined and move on to how to find them in your annotated corpus (REGEX)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Investigating language variation means that there is more than one way of saying the same thing. Speakers may vary pronunciation, morphology, word choice, etc.\n",
    "\n",
    "In linguistic research, we usually work with a number of variables (dependent and independent). \n",
    "In simple terms, an independent variable is the “input” (controlled factor) and a dependent variable is what results from the set of independent variables as an \"output\" (outcomes being measured). On the one hand, an independent variable is what is given (e.g. age, sex, education). On the other hand, a dependent variable is what results from the set of independent variables (e.g. pronunciation of a phoneme, morpheme or words).\n",
    "\n",
    "### Defining variables with JSON\n",
    "When annotating your data, you will for sure use some variables. These variables may have a hierarchical structure, where one category includes many variations. To allow the program to find your variables you have to build a \"dictionary\" where you specify them.\n",
    "Here, we use JSON files that allow you to come up with how many categories and variables you want in a clear and defined manner. Check out the [introduction to JSON tutorial](https://www.w3schools.com/js/js_json_intro.asp) if you are not familiar with it.\n",
    "You can also look at the  [dependent variables](./dependent_variables.json) we are using in this project for an example.\n",
    "\n",
    "### Your files\n",
    "Now that you are familiar with how the JSON syntax work, we need to define what your variables are. Here you have two options, depending on the option you choose you will need to run different cells.\n",
    "\n",
    "##### 1. Use the default variables file\n",
    "At the moment there are two files, one for [dependent variables](./dependent_variables.json) and one for [independent variables](./independent_variables.json) in this code. If you open these link you will see the original files (not modificable). If you want to modify them you need to open them from this page. On the folder symbol on the left of this code (check [this video](./includes/changing_variables.gif) for a how-to, and  [this tutorial](https://neptune.ai/blog/google-colab-dealing-with-files) on how to access local files system to google colab, point 4), click on CorpusCompass (this will open the direcotry) and then you will see the two files. Click on them to modify their content.\n",
    "\n",
    "##### 2. Upload your own files\n",
    "Here you can upload your own variable files as long as they are still in JSON format. You can upload as many files as you want.\n",
    "\n",
    "##### Choose your method\n",
    "You can choose either method 1 or 2, by changing the value of `variable_method` to be 1 or 2. By defualt the method is set to 1 (using the variables defined here).\n",
    "\n",
    "It should look like this `variable_method=1` or this `variable_method=2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment for different methods\n",
    "\n",
    "variable_method=1\n",
    "\n",
    "assert variable_method in [1,2], f\"Invalid method number {variable_method}! Choose between 1 and 2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if variable_method==1:\n",
    "    # method 1, load from files\n",
    "    dependent_variable_path = 'CorpusCompass/dependent_variables.json'\n",
    "    independent_variable_path = 'CorpusCompass/independent_variables.json'\n",
    "\n",
    "    variable_files=[dependent_variable_path, independent_variable_path]\n",
    "    variable_files=[open(path, \"r+\") for path in variable_files]\n",
    "    variable_files=[x.read() for x in variable_files]\n",
    "else:\n",
    "    # method 2, upload files\n",
    "    variable_files=files.upload()\n",
    "\n",
    "    for fn in variable_files.keys():\n",
    "        if \".json\" not in fn:\n",
    "            raise FileNotFoundError(f\"File {fn} is not a JSON file!\")\n",
    "        print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "          name=fn, length=len(variable_files[fn])))\n",
    "\n",
    "    variable_files=list(variable_files.values())\n",
    "\n",
    "    # decode\n",
    "    variable_files=[x.decode(\"utf8\") for x in variable_files]\n",
    "\n",
    "# show variables\n",
    "variable_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions (REGEX)\n",
    "\n",
    "Now it's time to define the regular expressions (REGEX) to find your annotation in the corpus.\n",
    "\n",
    "In this example, each annotation is contained in square brackets and starts with a dollar sign. The variables are divided by a dot (without spaces) and the last element after the dot is the annotated word (with spaces).\n",
    "Formally : \n",
    "\n",
    "```[$variable1.variable2.annotated word]```\n",
    "\n",
    "An example: \n",
    "\n",
    "`[$verb.ing.playing]`\n",
    "\n",
    "\n",
    "If your annotations follow a different rule, you need to come up with a new REGEX.\n",
    "\n",
    "You can use [regex101](https://regex101.com/) for this. Follow these instructions to test it:\n",
    "- Open  [regex101](https://regex101.com/)\n",
    "- Copy-paste the content of the `square_regex` (defined below) (`(\\[\\$[\\S ]*?\\])`) into the regular expression bar (on the top)\n",
    "- Copy paste a sample paragraph in the text string (on the bottom), e.g.:\n",
    "```\n",
    "A: akiid aani asawwi  [$G-JOB.awsbildung]  bass igulluuli innu l ʕaluum il baaylooji yaʕni li huwwa taħlilaat ṣaʕub bass iða ṣaʕub asawwi ɣeera akiid yaʕni, innu musaaʕadat doktoor asnaan ħaaba  [$TYP-IA.hammeen], haaða ʃ ʃii [$TYP-IA.hamma] aah, ey waḷḷa, aḷḷa ysallmič  ʕindi tlaθ aṭfaal, aah, il ʕindi ṭifla čibiira sitt isniin bi l madrasa uw ʕali ʕumra tlaθ isniin, arbaʕ isniin [$G-EDU.kindargaartin] uw samiir santeen uw nuṣṣ, santeen uw θmann iʃhuur [$G-EDU.kindarkriipa]\n",
    "\n",
    "```\n",
    "\n",
    "Define our REGEXs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex to find the complete annotation rule\n",
    "square_regex = re.compile(r\"(\\[\\$[\\S ]*?\\])\")\n",
    "# regex to find the content of an annotation\n",
    "feat_regex = re.compile(r'\\[\\$([\\S ]*?)\\]')\n",
    "# regex to find the token in the annotation\n",
    "annotated_token_regex=re.compile(r\"\\.[\\S ]*?\\]\")\n",
    "# regex to univocally finding the speaker name in the paragraph\n",
    "# uncomment if you don't have speakers at the start of each paragraph\n",
    "# name_regex= re.compile(r\"^\")\n",
    "name_regex= re.compile(r\"(^[A-Z]): \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus file\n",
    "Finally, you need to upload your corpus file. This is the file containing your annotated tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change use_example_corpus to False, in order to upload your own corpus\n",
    "# use_example_corpus=False\n",
    "use_example_corpus=True\n",
    "\n",
    "\n",
    "if use_example_corpus:\n",
    "    import pathlib\n",
    "    cur_path=pathlib.Path().resolve()\n",
    "    corpus_path=cur_path.joinpath(\"CorpusCompass/includes/corpus_example.txt\")\n",
    "    with open(corpus_path, \"r+\", encoding=\"utf8\") as f:\n",
    "        corpus_text=f.read()\n",
    "else:\n",
    "    corpus_path = files.upload()\n",
    "    corpus_text=multi_corpus_upload(corpus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\\n First 400 characters:\\n\")\n",
    "corpus_text[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output file\n",
    "\n",
    "In order to obtain the comma separated values (`csv`) file, you can specify the separator. By default the separator value is a comma (as the name implies), but you can also use:\n",
    "- semicolon: `;`\n",
    "- comma: `,`\n",
    "- tab : `\\t`\n",
    "\n",
    "Be aware that having the same character in your corpus may break the `csv` visualization. We suggest using a symbol that does not appear in your corpus, and then manually set the separator in the program you are using to visualize (e.g. excel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "separator = ';'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program\n",
    "Time to start the main program. First let us merge all the variables we uploaded before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_files=[json.loads(f) for f in variable_files]\n",
    "\n",
    "variable_dict = {}\n",
    "\n",
    "for j in variable_files:\n",
    "    for k,v in j.items():\n",
    "        if k in variable_dict.keys():\n",
    "            print(f\"Warning : I have found a duplicate variable named '{k}' with values '{v}'. \"\n",
    "                  f\"If this is expected ignore this message, otherwise check the variables files for duplicates!\")\n",
    "        variable_dict[k]=v\n",
    "\n",
    "\n",
    "print(\"Correctly loaded all the variables. Check them out to see if there are any errors\")\n",
    "print(json.dumps(variable_dict,sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing transcriptions\n",
    "Based on your type of annotated corpus you will need to preprocess the file.\n",
    "Feel free to comment out (add `#` at the start of the line) any preprocessing that does not fit your criteria.\n",
    "To give you a sense of our transcriptions, here is how the first paragraphs look like:\n",
    "\n",
    "```\n",
    "A: akiid aani asawwi  [$G-JOB.awsbildung]  bass igulluuli innu l ʕaluum il baaylooji yaʕni li huwwa taħlilaat ṣaʕub bass iða ṣaʕub asawwi ɣeera akiid yaʕni, innu musaaʕadat doktoor asnaan ħaaba  [$TYP-IA.hammeen], haaða ʃ ʃii [$TYP-IA.hamma] aah, ey waḷḷa, aḷḷa ysallmič  ʕindi tlaθ aṭfaal, aah, il ʕindi ṭifla čibiira sitt isniin bi l madrasa uw ʕali ʕumra tlaθ isniin, arbaʕ isniin [$G-EDU.kindargaartin] uw samiir santeen uw nuṣṣ, santeen uw θmann iʃhuur [$G-EDU.kindarkriipa]\n",
    "\n",
    "B: sawweet yaʕni baarħa innu gidarit adrus bi l leel ʕala muud awaffir waqti l yoom ilkum  ayy, ʃukran ilič tislamiin  la, [$G-DL.rootiin] il yoomi ʕindi dawaam aani daaʔiman min iθ θmaaniya w nuṣṣ li s saaʕa arbaʕa asawwi [$G-JOB.awsbildung] tamriiḍ li huwwa [$G-JOB.kraankenbifleegaaa] baʕd id dawaam arjaʕ mumkin asawwi [$G-DL.aaynkawfin] loo maθalan asawwi [$G-DL.teermiin] aw ʃii baʕadeen arjaʕ li l beet, mumkin aṭbux, aakul, leen aani saakna waħdi fa ʃwayya tkuun ṣaʕba bi n nisba ili\n",
    "\n",
    "C: ħiluw aʃyaaʔ [$TYP-IA.aku] ħilwa aa bass atmanna n naas yaʕni, aa [$TYP-IA.Q-WORD.ʃoon]  yistaxdimuuha b ṣuura ṣaħiiħa ey haaða yaʕni il mafruuḍ il kull tfakkir bii ey hm, naʕam, naʕam, naʕam, hm bali, ṣaħħ naʕam, ey ey ey, ṣaħiiħ, ey ṣaħħ, ṣaħiiħ ey ħamd-il-laa ey, naʕam, hmm la, la ṭabʕan akiid, ma, yaʕni ṣaar, aa,  [$TYP-IA.fadd]  ʃii wiyya l ħayaat yaʕni [$TYP-IA.Q-WORDʃoon] ka anna ma ʃurbat il ṃayy ma tgidriin, yaʕni aani ḍiʕit\n",
    "\n",
    "D: yaʕni n naas aa, mitfahhimiin iʃ ʃii uw yaʕni ma [$TYP-IA.da] ydaxxiluun iʃ ʃaɣḷaat yaʕni ḍiddhum haaði fa huwwa l ʕeeb, il ʕeeb bi l baʃar illi il ʕarabi il [$TYP-IA.da] yiji ey, aha, bi ḍ ḍaBiṭ, naʕam bi ḍ ḍaBiṭ la waḷḷa [$TYP-IA.da] ruuħ aa, bass aa l [$G-EDU.koors] xiḷas, ey ma ṭaḷḷaʕit natiija yaʕni, ey ey, ey waḷḷa, waḷḷa yaʕni aa, min, aa xijalit min nafsi yaʕni, gumit aa, ħatta ħatta nafsiiti tiʕbat yaʕni aa, yaʕni ħatta l ħijiyya uw aṣdiqaaʔi yaʕni, igulli ʕammu [$TYP-IA.Q-WORD.ʃinu] yaʕni\n",
    "\n",
    "```\n",
    "As you can see our file has a repeating structure of the kind:\n",
    "- speaker name (`A`,`B`,`C`,`D`), space, paragraph, newline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: split the whole corpus in different elements every new line, creating a list of paragraphs. For us this means splitting interviewer and interviewee in different paragraphs\n",
    "corpus= split_paragraphs(corpus_text)\n",
    "# step 2: remove spaces at the start and end of each paragraph\n",
    "corpus = [x.strip() for x in corpus]\n",
    "# step 3 : remove empty paragraphs from the list\n",
    "corpus = [x for x in corpus if x != '']\n",
    "# step 4 : filter out all the paragraphs that do not have any DETECTED speaker\n",
    "prev_c=copy(corpus)\n",
    "corpus=[x for x in corpus if get_name(x,name_regex)]\n",
    "\n",
    "removed_sentences=len(prev_c)-len(corpus)\n",
    "print(f\"I removed {removed_sentences} paragraphs, since I could not detect a speaker\\n\"\n",
    "      f\"I will show it over here, sorted by the their line:\")\n",
    "if removed_sentences>0:\n",
    "    diff=set(prev_c)-set(corpus)\n",
    "    diff=sorted(diff,key=lambda x: prev_c.index(x))\n",
    "    for i in diff:\n",
    "        print(f\"{prev_c.index(i)}: {i}\")\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"If you see paragraphs you are interested in, consider manually changing them in the corpus, or expanding the 'name_regex' rule\")\n",
    "\n",
    "\n",
    "if len(corpus)==0:\n",
    "    print(\"All the paragraphs in the corpus have been deleted! You should review your regex rules\")\n",
    "else:\n",
    "    # take a peek at the first four paragraphs\n",
    "    print(\"First paragraphs are\")\n",
    "    for idx in range(3):\n",
    "        print(corpus[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speaker of interest\n",
    "As you can see form the previous example, we consider a file where we have two speakers alternating each other. But it can be the case where you have multiple in random order. In this case you can specify the names of the speakers you are interested in, notice that the others will be skipped from the final output.\n",
    "\n",
    "If you need to add multiple speakers run the following cell after adding the names. It should look something like:\n",
    "\n",
    "\n",
    "`speakers_of_interest = \"name1,name2,...,nameN\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_of_interest=\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the interviewers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if user specified the interviewer's names then take that\n",
    "if len(speakers_of_interest) > 0:\n",
    "    speakers_of_interest = speakers_of_interest.split(',')\n",
    "    # remove spaces\n",
    "    speakers_of_interest = [x.strip() for x in speakers_of_interest]\n",
    "else:\n",
    "    # else use the first character of the transcription\n",
    "    speakers_of_interest = [corpus[1][0]]\n",
    "     # filter out empty names\n",
    "    speakers_of_interest = [x for x in speakers_of_interest if x != '']\n",
    "    speakers_of_interest=[speakers_of_interest[0]]\n",
    "\n",
    "print(f\"The selected speakers of interest are: {', '.join(speakers_of_interest)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding all speakers\n",
    "\n",
    "This part looks for all the names present in the file and checks the independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get interviewer/interviewees names\n",
    "all_speakers = [get_name(x, name_regex).strip() for x in corpus]\n",
    "all_speakers = set(all_speakers)\n",
    "# filter out empty all_speakers\n",
    "all_speakers = [x for x in all_speakers if x != '']\n",
    "\n",
    "# notify user about names\n",
    "print(f\"I found the following speakers names: {', '.join(all_speakers)}\")\n",
    "\n",
    "\n",
    "# get the independent variables\n",
    "independent_variable_dict={k:v for k,v in variable_dict.items() if k in all_speakers}\n",
    "# remove from variable_dict the independent variables\n",
    "variable_dict={k:v for k,v in variable_dict.items() if k not in all_speakers}\n",
    "\n",
    "\n",
    "# get an inverse of the dependent variable\n",
    "idv = {}\n",
    "for k, v in variable_dict.items():\n",
    "    if isinstance(v, list):\n",
    "        for i in v:\n",
    "            idv[i] = k\n",
    "    else:\n",
    "        idv[v] = k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output file settings\n",
    "Now it's time to create the output files.\n",
    "\n",
    "There are different outputs and here we specify their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the name of the output path\n",
    "dataset_path=\"dataset.csv\"\n",
    "annoation_info_path=\"annotation_info.csv\"\n",
    "not_annotated_path=\"not_annotated_log.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be useful to add the previous paragraph in the final csv file. For example, when examining an annotation, you want to know what the previous speaker said before the current one (turn taking). If you are interested in this information being in the final output set `previous_line` to `True`, else leave it `False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_line = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we include a context for all the annotated tokens present in your data. The context is made of the previous/next words after the annotated one, following a n-gram rule as you can see in the rule below: \n",
    "\n",
    "`ngram_params = (number previous words, number next words)`\n",
    "\n",
    "Here we decided to take into account 10 words appearing before the annotated one and 5 after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_params=(10,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output is a csv file and it needs a header. For this reason, here we define the header with the following elements:\n",
    "- The `token` for the annotated words\n",
    "- The complete list of your variables\n",
    "- (Optional) the previous line\n",
    "- The speaker's name\n",
    "- The context in which the token was found\n",
    "- An `unk` (unknown) category for variables that were found in the annotations but not present in the variable files (useful to catch some errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile regex to find features\n",
    "csv_header = list(variable_dict.keys())\n",
    "\n",
    "# define the end of the csv\n",
    "csv_end = [\"speaker\",'context', 'unk']\n",
    "if previous_line:\n",
    "    csv_end.insert(0, 'previous line')\n",
    "csv_header = [\"token\"] + csv_header + csv_end\n",
    "csv_file = [csv_header]\n",
    "unk_categories = []\n",
    "\n",
    "print(f\"The csv header looks like this\")\n",
    "csv_header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotation information\n",
    "\n",
    "Here we build our counter and logger for the annotations. We want to count the frequency of each annotated token in the corpus, how many times it appears and who uses it. Moreover, for logging reasons, we want to present the tokens which are annotated but were found outside the annotation rule (not recognized)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ### Finding all the annotated words\n",
    "whole_corpus=\"\\n\".join(corpus)\n",
    "annotations=feat_regex.findall(whole_corpus)\n",
    "annotations=[x.split(\".\")[-1] for x in annotations]\n",
    "\n",
    "# count the number of annotations\n",
    "annotation_counter = Counter(annotations)\n",
    "annotation_counter={k:dict(annotated=v) for k,v in annotation_counter.items()}\n",
    "\n",
    "print(f\"The total number of annotated words is {len(annotation_counter)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we check the number of times the token appears annotated (following the REGEX rule) vs not, we do this for each annotated token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# check if there are any annotations not annotated\n",
    "for k,v in annotation_counter.items():\n",
    "    # check for annotation repetitions\n",
    "    wild_rep,ann_rep,_= find_repetitions(whole_corpus, k, annotated_token_regex)\n",
    "    total_rep=wild_rep+ann_rep\n",
    "    not_annotated=total_rep-v['annotated']\n",
    "    annotation_counter[k]['not_annotated']=not_annotated\n",
    "\n",
    "print(f\"The total repetitions of annotated words is {sum([x['annotated'] for x in annotation_counter.values()])}\")\n",
    "print(f\"The total repetitions of not annotated words is {sum([x['not_annotated'] for x in annotation_counter.values()])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the previous cell, counts all the tokens in the whole corpus, we probably want to differentiate between speakers.\n",
    "For this reason, here we need to specify where we want to look for the missed tokens.\n",
    "\n",
    "You have the following options:\n",
    "\n",
    "- Choose all the speakers found in your corpus with: `speakers=all_speakers`\n",
    "- Choose only the speakers you are interested in (defined previously):\n",
    "`speakers=speakers_of_interest`\n",
    "- Choose other speakers you are interested in, by manually enumerating them: `speakers= [\"name1\",\"name2\",...,\"nameN\"]`\n",
    "\n",
    "By default, we check for all the speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose which speaker to check for annotations, you can uncomment one of the following lines:\n",
    "speakers=all_speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to find those missing annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_annotated_log={}\n",
    "\n",
    "for k,v in annotation_counter.items():\n",
    "\n",
    "    res=[find_repetitions(x, k, annotated_token_regex) for x in corpus if get_name(x,name_regex) in speakers]\n",
    "    _,_,wild_not_annotated=zip(*res)\n",
    "\n",
    "    # unfold the list\n",
    "    wild_not_annotated=[item for sublist in wild_not_annotated for item in sublist]\n",
    "\n",
    "    # if there are not annotated words\n",
    "    if len(wild_not_annotated) > 0:\n",
    "        not_annotated_log[k]=wild_not_annotated\n",
    "\n",
    "print(f\"The total number of not annotated words is {len(not_annotated_log)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, some pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# augment annotation_counter with speakers and add total number\n",
    "for k in annotation_counter.keys():\n",
    "    annotation_counter[k]['total'] = annotation_counter[k]['annotated'] + annotation_counter[k]['not_annotated']\n",
    "    for speaker in all_speakers:\n",
    "        annotation_counter[k][speaker] = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting the main loop\n",
    "This part starts the main loop. You don't need to change anything here, if you are interested check out the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for every paragraph in the transcript\n",
    "for idx in range(len(corpus)):\n",
    "    c = corpus[idx]\n",
    "    cur_speaker = get_name(c,name_regex)\n",
    "\n",
    "    # add speaker related metrics\n",
    "    for k, v in annotation_counter.items():\n",
    "        wild_rep, ann_rep, _ = find_repetitions(c, k, annotated_token_regex)\n",
    "        rep = wild_rep + ann_rep\n",
    "        annotation_counter[k][cur_speaker] += rep\n",
    "\n",
    "    # get the paragraph without features\n",
    "    if cur_speaker in speakers_of_interest:\n",
    "        sp = corpus[idx - 1]\n",
    "    else:\n",
    "        continue\n",
    "    clean_p = remove_features(c, square_regex)\n",
    "\n",
    "    # get the features\n",
    "    tags = feat_regex.finditer(c)\n",
    "\n",
    "    # for every tags with features in the paragraph\n",
    "    for t in tags:\n",
    "        # get index of result + tag\n",
    "        index = t.start()\n",
    "        t = t.group(1)\n",
    "\n",
    "        # initialize empty row\n",
    "        csv_line = [\"\" for _ in range(len(csv_header))]\n",
    "\n",
    "         # get independent variable information\n",
    "        for k, v in independent_variable_dict.items():\n",
    "            if cur_speaker!=k:\n",
    "                continue\n",
    "            for var in v:\n",
    "\n",
    "                category = idv[var]\n",
    "                cat_idx = csv_header.index(category)\n",
    "                csv_line[cat_idx] = var\n",
    "\n",
    "\n",
    "        # get the features\n",
    "        feats = t.rsplit(\".\", 1)\n",
    "        text = feats[1]\n",
    "        feats = feats[0]\n",
    "\n",
    "        context=get_ngram(text,clean_p,ngram_params)\n",
    "\n",
    "        # for every feature in the word\n",
    "        for f in feats.split(\".\"):\n",
    "            # if the category is not present in the dict, then add to unk\n",
    "            if f not in idv.keys():\n",
    "                unk_categories.append(f)\n",
    "                csv_line[-1] = csv_line[-1] + f + \",\"\n",
    "            else:\n",
    "                category = idv[f]\n",
    "                cat_idx = csv_header.index(category)\n",
    "                csv_line[cat_idx] = f\n",
    "\n",
    "        # add initial infos and final unk to the line\n",
    "        csv_line[0] = text\n",
    "        csv_line[-2] = context\n",
    "        csv_line[-3] = cur_speaker\n",
    "        if previous_line:\n",
    "            csv_line[-4] = sp\n",
    "\n",
    "        csv_line[-1] = csv_line[-1].strip(\",\")\n",
    "        csv_file.append(csv_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the output\n",
    "Finally, we need to save the output in the csv file for all our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# write the csv\n",
    "with open(dataset_path, \"w\", newline=\"\", encoding=\"utf16\") as f:\n",
    "    writer = csv.writer(f, delimiter=separator)\n",
    "    writer.writerows(csv_file)\n",
    "\n",
    "\n",
    "# generate the annotation info file\n",
    "header=[\"token\"]+list(list(annotation_counter.values())[0].keys())\n",
    "\n",
    "with open(annoation_info_path, \"w\", newline=\"\", encoding=\"utf16\") as f:\n",
    "    writer = csv.writer(f, delimiter=separator)\n",
    "    writer.writerow(header)\n",
    "\n",
    "    for k,v in annotation_counter.items():\n",
    "        writer.writerow([k]+list(v.values()))\n",
    "\n",
    "\n",
    "# save the not annotated log\n",
    "header=[\"token\"]+[f\"context {i}\" for i in range(max(len(x) for x in not_annotated_log.values()))]\n",
    "with open(not_annotated_path, \"w\", newline=\"\", encoding=\"utf16\") as f:\n",
    "    writer = csv.writer(f, delimiter=separator)\n",
    "    writer.writerow(header)\n",
    "    for k, v in not_annotated_log.items():\n",
    "        writer.writerow([k]+v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to download it right away run this cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files.download(dataset_path)\n",
    "files.download(annoation_info_path)\n",
    "files.download(not_annotated_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unknown categories\n",
    "Here, we show the unknown category, if any could be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if len(unk_categories) > 0:\n",
    "    unk_categories = set(unk_categories)\n",
    "    unk_categories = sorted(unk_categories)\n",
    "    print(print(\n",
    "        f\"I have found several categories not listed in your variable file.\\n\"\n",
    "        f\"Following in alphabetical order:\"))\n",
    "    for idx,c in enumerate(unk_categories):\n",
    "        print(idx,f\"'{c}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
